{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Mesonet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "# your code here\n",
    "\n",
    "\n",
    "# Define the input directory for 5-minute rainfall data\n",
    "path_5min_rainfall = os.path.join(os.getcwd(), \"RAW\")\n",
    "\n",
    "# Define the output directory for saving storm data\n",
    "path_each_station = os.path.join(os.getcwd(), \"each_site\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(path_each_station, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for file_path in glob.glob(os.path.join(path_5min_rainfall, '*.csv')):\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for required columns\n",
    "        required_columns = {'STID', 'TIME', 'RAIN'}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            print(f\"Skipping file {file_path}. Missing required columns.\")\n",
    "            continue\n",
    "\n",
    "        # Extract unique station IDs\n",
    "        unique_stations = df['STID'].unique()\n",
    "\n",
    "        # Loop through each unique station, filter the DataFrame, and save to a new CSV\n",
    "        for station in unique_stations:\n",
    "            # Filter the DataFrame for the current station\n",
    "            df_station = df[df['STID'] == station][['STID', 'TIME', 'RAIN']]\n",
    "\n",
    "            # Define the output file path using the station name, safely handle file names\n",
    "            output_file_name = f\"{station.replace('/', '_')}.csv\"  # Replace '/' with '_' to avoid path issues\n",
    "            output_file_path = os.path.join(path_each_station, output_file_name)\n",
    "\n",
    "            # Save the filtered DataFrame to a CSV file\n",
    "            df_station.to_csv(output_file_path, index=False)\n",
    "            print(f\"Saved data for station {station} to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"All files have been processed and saved.\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Cell ran in {elapsed_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: OKEM\n",
      "✅ Done: WIST\n",
      "✅ Done: STUA\n",
      "✅ Done: SALL\n",
      "✅ Done: ADAX\n",
      "✅ Done: ANT2\n",
      "✅ Done: CARL\n",
      "✅ Done: BEAV\n",
      "✅ Done: WASH\n",
      "✅ Done: NRMN\n",
      "✅ Done: KENT\n",
      "✅ Done: ELKC\n",
      "✅ Done: FREE\n",
      "✅ Done: ELRE\n",
      "✅ Done: BURN\n",
      "✅ Done: PUTN\n",
      "✅ Done: GRA2\n",
      "✅ Done: SEMI\n",
      "✅ Done: ALTU\n",
      "✅ Done: EUFA\n",
      "✅ Done: NOWA\n",
      "✅ Done: MEDF\n",
      "✅ Done: NEWK\n",
      "✅ Done: TALA\n",
      "✅ Done: WILB\n",
      "✅ Done: BESS\n",
      "✅ Done: OILT\n",
      "✅ Done: APAC\n",
      "✅ Done: MANG\n",
      "✅ Done: HOLL\n",
      "✅ Done: LAHO\n",
      "✅ Done: STIL\n",
      "✅ Done: RING\n",
      "✅ Done: SHAW\n",
      "✅ Done: HINT\n",
      "✅ Done: WEB3\n",
      "✅ Done: BRIS\n",
      "✅ Done: WATO\n",
      "✅ Done: TAHL\n",
      "✅ Done: CHEY\n",
      "✅ Done: COOK\n",
      "✅ Done: BYAR\n",
      "✅ Done: SLAP\n",
      "✅ Done: TIPT\n",
      "✅ Done: MCAL\n",
      "✅ Done: HOBA\n",
      "✅ Done: PAWN\n",
      "✅ Done: BREC\n",
      "✅ Done: TULN\n",
      "✅ Done: SULP\n",
      "✅ Done: BUTL\n",
      "✅ Done: OKMU\n",
      "✅ Done: TISH\n",
      "✅ Done: PRYO\n",
      "✅ Done: SPEN\n",
      "✅ Done: BOIS\n",
      "✅ Done: CENT\n",
      "✅ Done: INOL\n",
      "✅ Done: FTCB\n",
      "✅ Done: HUGO\n",
      "✅ Done: SKIA\n",
      "✅ Done: CHER\n",
      "✅ Done: HOLD\n",
      "✅ Done: JAYX\n",
      "✅ Done: FITT\n",
      "✅ Done: IDAB\n",
      "✅ Done: YUKO\n",
      "✅ Done: STIG\n",
      "✅ Done: DURA\n",
      "✅ Done: HOOK\n",
      "✅ Done: BLAC\n",
      "✅ Done: WEAT\n",
      "✅ Done: OKCE\n",
      "✅ Done: SEIL\n",
      "✅ Done: ARNE\n",
      "✅ Done: MAYR\n",
      "✅ Done: HECT\n",
      "✅ Done: CAMA\n",
      "✅ Done: WAL2\n",
      "✅ Done: GUTH\n",
      "✅ Done: CHIC\n",
      "✅ Done: BBOW\n",
      "✅ Done: WAUR\n",
      "✅ Done: PAUL\n",
      "✅ Done: MIAM\n",
      "✅ Done: WOOD\n",
      "✅ Done: ACME\n",
      "✅ Done: MINC\n",
      "✅ Done: ERIC\n",
      "✅ Done: COPA\n",
      "✅ Done: CHAN\n",
      "✅ Done: HASK\n",
      "✅ Done: LANE\n",
      "✅ Done: NEWP\n",
      "✅ Done: VINI\n",
      "✅ Done: PERK\n",
      "✅ Done: KIN2\n",
      "✅ Done: ARD2\n",
      "✅ Done: GOOD\n",
      "✅ Done: BURB\n",
      "✅ Done: CLOU\n",
      "✅ Done: MEDI\n",
      "✅ Done: PORT\n",
      "✅ Done: ALV2\n",
      "✅ Done: BUFF\n",
      "✅ Done: MARE\n",
      "✅ Done: BIXB\n",
      "✅ Done: MRSH\n",
      "✅ Done: REDR\n",
      "✅ Done: MADI\n",
      "✅ Done: MTHE\n",
      "✅ Done: KETC\n",
      "✅ Done: VALL\n",
      "✅ Done: TALI\n",
      "✅ Done: CLAY\n",
      "✅ Done: FORA\n",
      "✅ Done: EVAX\n",
      "✅ Done: FAI2\n",
      "✅ Done: WEST\n",
      "✅ Done: WYNO\n",
      "🎉 Finished splitting all stations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ paths ------------\n",
    "input_dir = os.path.join(os.getcwd(), \"each_site\")\n",
    "output_root = os.path.join(os.getcwd(), \"Rain_Data\")\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "START_YEAR, END_YEAR = 1994, 2024\n",
    "\n",
    "# ------------ processing ------------\n",
    "csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "if not csv_files:\n",
    "    print(f\"No CSV files found in {input_dir}\")\n",
    "\n",
    "for fp in csv_files:\n",
    "    stid = os.path.splitext(os.path.basename(fp))[0]  # filename == STID\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {fp}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"TIME\" not in df.columns:\n",
    "        print(f\"⚠️ No 'TIME' column found in {fp}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Parse TIME column (format: 1994-01-01T00:00)\n",
    "    df[\"TIME\"] = pd.to_datetime(df[\"TIME\"], format=\"%Y-%m-%dT%H:%M\", errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"TIME\"]).copy()\n",
    "\n",
    "    # Filter to requested year range\n",
    "    df[\"year\"] = df[\"TIME\"].dt.year\n",
    "    df[\"month\"] = df[\"TIME\"].dt.month\n",
    "    df = df[(df[\"year\"] >= START_YEAR) & (df[\"year\"] <= END_YEAR)]\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"ℹ️ No rows within {START_YEAR}-{END_YEAR} for {stid}.\")\n",
    "        continue\n",
    "\n",
    "    # Convert RAIN from inch → mm\n",
    "    if \"RAIN\" in df.columns:\n",
    "        df[\"RAIN\"] = pd.to_numeric(df[\"RAIN\"], errors=\"coerce\") * 25.4\n",
    "\n",
    "    # Create station folder\n",
    "    stid_dir = os.path.join(output_root, stid)\n",
    "    os.makedirs(stid_dir, exist_ok=True)\n",
    "\n",
    "    # Group by year and month\n",
    "    for (yr, mo), g in df.groupby([\"year\", \"month\"]):\n",
    "        year_dir = os.path.join(stid_dir, f\"{yr:04d}\")\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        out_name = f\"{stid}_{yr:04d}{mo:02d}.csv\"\n",
    "        out_path = os.path.join(year_dir, out_name)\n",
    "\n",
    "        # Rename columns to lowercase\n",
    "        g = g.rename(columns={\"STID\": \"stid\", \"TIME\": \"time\", \"RAIN\": \"rain\"})\n",
    "\n",
    "        # Save without helper columns\n",
    "        g.drop(columns=[\"year\", \"month\"]).to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"✅ Done: {stid}\")\n",
    "\n",
    "print(\"🎉 Finished splitting all stations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the recording period for each site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the recording length based on 'RAIN' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STID</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Start Year</th>\n",
       "      <th>End Year</th>\n",
       "      <th>Period Days</th>\n",
       "      <th>Period Years</th>\n",
       "      <th>Valid Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1994-02-18</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11275</td>\n",
       "      <td>30.87</td>\n",
       "      <td>3213630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADAX</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11323</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3216965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALTU</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11323</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3193774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALV2</td>\n",
       "      <td>1998-12-17</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1998</td>\n",
       "      <td>2025</td>\n",
       "      <td>9512</td>\n",
       "      <td>26.04</td>\n",
       "      <td>2679820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANT2</td>\n",
       "      <td>2011-04-15</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2011</td>\n",
       "      <td>2025</td>\n",
       "      <td>5010</td>\n",
       "      <td>13.72</td>\n",
       "      <td>1433795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>WILB</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11323</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3189699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>WIST</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11323</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3238740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>WOOD</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11323</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3221234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>WYNO</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1994</td>\n",
       "      <td>2025</td>\n",
       "      <td>11323</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3240335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>YUKO</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2018</td>\n",
       "      <td>2025</td>\n",
       "      <td>2388</td>\n",
       "      <td>6.54</td>\n",
       "      <td>676481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     STID  Start Date    End Date  Start Year  End Year  Period Days  \\\n",
       "0    ACME  1994-02-18  2025-01-01        1994      2025        11275   \n",
       "1    ADAX  1994-01-01  2025-01-01        1994      2025        11323   \n",
       "2    ALTU  1994-01-01  2025-01-01        1994      2025        11323   \n",
       "3    ALV2  1998-12-17  2025-01-01        1998      2025         9512   \n",
       "4    ANT2  2011-04-15  2025-01-01        2011      2025         5010   \n",
       "..    ...         ...         ...         ...       ...          ...   \n",
       "116  WILB  1994-01-01  2025-01-01        1994      2025        11323   \n",
       "117  WIST  1994-01-01  2025-01-01        1994      2025        11323   \n",
       "118  WOOD  1994-01-01  2025-01-01        1994      2025        11323   \n",
       "119  WYNO  1994-01-01  2025-01-01        1994      2025        11323   \n",
       "120  YUKO  2018-06-18  2025-01-01        2018      2025         2388   \n",
       "\n",
       "     Period Years  Valid Records  \n",
       "0           30.87        3213630  \n",
       "1           31.00        3216965  \n",
       "2           31.00        3193774  \n",
       "3           26.04        2679820  \n",
       "4           13.72        1433795  \n",
       "..            ...            ...  \n",
       "116         31.00        3189699  \n",
       "117         31.00        3238740  \n",
       "118         31.00        3221234  \n",
       "119         31.00        3240335  \n",
       "120          6.54         676481  \n",
       "\n",
       "[121 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, station_periods\n",
    "importlib.reload(station_periods)\n",
    "calculate_station_periods = station_periods.calculate_station_periods  # rebind the updated function\n",
    "\n",
    "from station_periods import calculate_station_periods\n",
    "\n",
    "input_folder = \"Rain_Data\"\n",
    "output_csv = \"Result/station_periods.csv\"\n",
    "\n",
    "df_periods = calculate_station_periods(\n",
    "    root_dir=input_folder,\n",
    "    time_col=\"time\",\n",
    "    rain_col=\"rain\",\n",
    "    stid_col=\"stid\",\n",
    "    save_csv_to=output_csv\n",
    ")\n",
    "\n",
    "df_periods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the sites with percentage of missing data(NaN values) > 5%. Remove the NaN values in the rainfall data from rest of the sites, and extract the sites with recording started from 1995-01-01 to 2023-12-31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved overall to: Result/missing_overall.csv\n",
      "✅ Saved per-year (all sites) to: Result/missing_by_year_all_sites.csv\n",
      "✅ Per-site yearly CSVs in: Result/missing_by_year\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   STID  Start Date    End Date  Missing_Percentage\n",
       " 0  ACME  1994-01-01  2024-01-31                1.23\n",
       " 1  ADAX  1994-01-01  2024-01-31                1.13\n",
       " 2  ALTU  1994-01-01  2024-01-31                1.86\n",
       " 3  ALV2  1998-12-17  2024-01-31                1.94\n",
       " 4  ANT2  2011-04-15  2024-01-31                0.05,\n",
       "    STID  Year  Missing_Percentage\n",
       " 0  ACME  1994               13.16\n",
       " 1  ACME  1995                4.67\n",
       " 2  ACME  1996                6.28\n",
       " 3  ACME  1997                0.04\n",
       " 4  ACME  1998                0.16)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import missing_data\n",
    "\n",
    "importlib.reload(missing_data)  # if you edit the file and want to reload\n",
    "\n",
    "from missing_data import compute_missing_stats\n",
    "\n",
    "input_folder = \"Rain_Data\"     # root_dir (STID/YEAR/*.csv)\n",
    "result_folder = \"Result\"       # outputs go here\n",
    "\n",
    "overall_df, yearly_df = compute_missing_stats(\n",
    "    root_dir=input_folder,\n",
    "    result_dir=result_folder,\n",
    "    negatives_are_missing=True,   # treat rain<0 as missing\n",
    "    save_by_site=True             # also save one CSV per STID with yearly %s\n",
    ")\n",
    "\n",
    "overall_df.head(), yearly_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out Sites with High Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keeping 116 sites with Missing_Percentage ≤ 5.0%\n",
      "   Copied ACME\n",
      "   Copied ADAX\n",
      "   Copied ALTU\n",
      "   Copied ALV2\n",
      "   Copied ANT2\n",
      "   Copied ARD2\n",
      "   Copied ARNE\n",
      "   Copied BBOW\n",
      "   Copied BEAV\n",
      "   Copied BESS\n",
      "   Copied BIXB\n",
      "   Copied BLAC\n",
      "   Copied BOIS\n",
      "   Copied BREC\n",
      "   Copied BRIS\n",
      "   Copied BROK\n",
      "   Copied BUFF\n",
      "   Copied BURB\n",
      "   Copied BURN\n",
      "   Copied BUTL\n",
      "   Copied BYAR\n",
      "   Copied CAMA\n",
      "   Copied CARL\n",
      "   Copied CENT\n",
      "   Copied CHAN\n",
      "   Copied CHER\n",
      "   Copied CHEY\n",
      "   Copied CHIC\n",
      "   Copied CLAY\n",
      "   Copied CLOU\n",
      "   Copied COOK\n",
      "   Copied COPA\n",
      "   Copied DURA\n",
      "   Copied ELKC\n",
      "   Copied ELRE\n",
      "   Copied ERIC\n",
      "   Copied EUFA\n",
      "   Copied EVAX\n",
      "   Copied FAI2\n",
      "   Copied FITT\n",
      "   Copied FORA\n",
      "   Copied FREE\n",
      "   Copied FTCB\n",
      "   Copied GRA2\n",
      "   Copied GUTH\n",
      "   Copied HASK\n",
      "   Copied HINT\n",
      "   Copied HOBA\n",
      "   Copied HOLD\n",
      "   Copied HOLL\n",
      "   Copied HOOK\n",
      "   Copied HUGO\n",
      "   Copied IDAB\n",
      "   Copied INOL\n",
      "   Copied JAYX\n",
      "   Copied KENT\n",
      "   Copied KETC\n",
      "   Copied KIN2\n",
      "   Copied LAHO\n",
      "   Copied LANE\n",
      "   Copied MADI\n",
      "   Copied MANG\n",
      "   Copied MARE\n",
      "   Copied MAYR\n",
      "   Copied MCAL\n",
      "   Copied MEDF\n",
      "   Copied MEDI\n",
      "   Copied MIAM\n",
      "   Copied MINC\n",
      "   Copied MRSH\n",
      "   Copied MTHE\n",
      "   Copied NEWK\n",
      "   Copied NEWP\n",
      "   Copied NOWA\n",
      "   Copied NRMN\n",
      "   Copied OILT\n",
      "   Copied OKCE\n",
      "   Copied OKEM\n",
      "   Copied OKMU\n",
      "   Copied PAUL\n",
      "   Copied PAWN\n",
      "   Copied PERK\n",
      "   Copied PORT\n",
      "   Copied PRYO\n",
      "   Copied PUTN\n",
      "   Copied REDR\n",
      "   Copied SALL\n",
      "   Copied SEIL\n",
      "   Copied SEMI\n",
      "   Copied SKIA\n",
      "   Copied SLAP\n",
      "   Copied SPEN\n",
      "   Copied STIG\n",
      "   Copied STIL\n",
      "   Copied STUA\n",
      "   Copied SULP\n",
      "   Copied TAHL\n",
      "   Copied TALA\n",
      "   Copied TALI\n",
      "   Copied TIPT\n",
      "   Copied TISH\n",
      "   Copied TULN\n",
      "   Copied VALL\n",
      "   Copied VINI\n",
      "   Copied WAL2\n",
      "   Copied WASH\n",
      "   Copied WATO\n",
      "   Copied WAUR\n",
      "   Copied WEAT\n",
      "   Copied WEB3\n",
      "   Copied WEST\n",
      "   Copied WILB\n",
      "   Copied WIST\n",
      "   Copied WOOD\n",
      "   Copied WYNO\n",
      "   Copied YUKO\n",
      "\n",
      "📂 High-quality sites saved to: Rain_Data_High_Quality\n"
     ]
    }
   ],
   "source": [
    "import importlib, filter_high_quality_sites\n",
    "importlib.reload(filter_high_quality_sites)\n",
    "\n",
    "res = filter_high_quality_sites.filter_high_quality_sites(\n",
    "    missing_overall_csv=\"Result/missing_overall.csv\",\n",
    "    rain_data_root=\"Rain_Data\",\n",
    "    output_root=\"Rain_Data_High_Quality\",\n",
    "    threshold=5.0,\n",
    "    dry_run=False  # preview without copying\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------Computation-----------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storm Idenfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input_File</th>\n",
       "      <th>Output_File</th>\n",
       "      <th>Rows_Input</th>\n",
       "      <th>Rows_Kept</th>\n",
       "      <th>Pct_Kept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rain_Data_High_Quality/ACME/1994/ACME_199402.csv</td>\n",
       "      <td>storms_identification/ACME/1994/ACME_199402.csv</td>\n",
       "      <td>8064</td>\n",
       "      <td>169</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rain_Data_High_Quality/ACME/1994/ACME_199403.csv</td>\n",
       "      <td>storms_identification/ACME/1994/ACME_199403.csv</td>\n",
       "      <td>8928</td>\n",
       "      <td>274</td>\n",
       "      <td>3.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rain_Data_High_Quality/ACME/1994/ACME_199404.csv</td>\n",
       "      <td>storms_identification/ACME/1994/ACME_199404.csv</td>\n",
       "      <td>8640</td>\n",
       "      <td>192</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain_Data_High_Quality/ACME/1994/ACME_199405.csv</td>\n",
       "      <td>storms_identification/ACME/1994/ACME_199405.csv</td>\n",
       "      <td>8928</td>\n",
       "      <td>254</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain_Data_High_Quality/ACME/1994/ACME_199406.csv</td>\n",
       "      <td>storms_identification/ACME/1994/ACME_199406.csv</td>\n",
       "      <td>8640</td>\n",
       "      <td>81</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Input_File  \\\n",
       "0  Rain_Data_High_Quality/ACME/1994/ACME_199402.csv   \n",
       "1  Rain_Data_High_Quality/ACME/1994/ACME_199403.csv   \n",
       "2  Rain_Data_High_Quality/ACME/1994/ACME_199404.csv   \n",
       "3  Rain_Data_High_Quality/ACME/1994/ACME_199405.csv   \n",
       "4  Rain_Data_High_Quality/ACME/1994/ACME_199406.csv   \n",
       "\n",
       "                                       Output_File  Rows_Input  Rows_Kept  \\\n",
       "0  storms_identification/ACME/1994/ACME_199402.csv        8064        169   \n",
       "1  storms_identification/ACME/1994/ACME_199403.csv        8928        274   \n",
       "2  storms_identification/ACME/1994/ACME_199404.csv        8640        192   \n",
       "3  storms_identification/ACME/1994/ACME_199405.csv        8928        254   \n",
       "4  storms_identification/ACME/1994/ACME_199406.csv        8640         81   \n",
       "\n",
       "   Pct_Kept  \n",
       "0      2.10  \n",
       "1      3.07  \n",
       "2      2.22  \n",
       "3      2.84  \n",
       "4      0.94  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, identify_storms\n",
    "importlib.reload(identify_storms)  # re-read if you just edited the .py\n",
    "\n",
    "input_folder = \"Rain_Data_High_Quality\"\n",
    "output_folder = \"storms_identification\"\n",
    "\n",
    "summary_df = identify_storms.identify_storms(\n",
    "    input_dir=input_folder,\n",
    "    output_dir=output_folder,\n",
    "    rain_col=\"rain\",   # your column names are lowercase\n",
    "    time_col=\"time\",   # set to None if not present\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "summary_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate rainfall intensity in each time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/process_intervals.py:55: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import importlib, process_intervals\n",
    "importlib.reload(process_intervals)\n",
    "\n",
    "process_intervals.process_directory(\n",
    "    input_dir=\"storms_identification\",\n",
    "    output_dir=\"storm_interval_information\",\n",
    "    recursive=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/separate_storm_events.py:189: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[t_col] = pd.to_datetime(df[t_col], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>station_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>event_start</th>\n",
       "      <th>event_end</th>\n",
       "      <th>event_duration_min</th>\n",
       "      <th>event_total_depth_mm</th>\n",
       "      <th>event_peak_intensity_mm_hr</th>\n",
       "      <th>output_file</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME_199402.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1994-02-18 17:30:00</td>\n",
       "      <td>1994-02-18 17:40:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.54</td>\n",
       "      <td>15.24</td>\n",
       "      <td>single_storm/ACME/1994/ACME_1994_02_1.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACME_199402.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1994-02-19 18:25:00</td>\n",
       "      <td>1994-02-19 22:10:00</td>\n",
       "      <td>225.0</td>\n",
       "      <td>10.16</td>\n",
       "      <td>51.82</td>\n",
       "      <td>single_storm/ACME/1994/ACME_1994_02_2.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACME_199402.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1994-02-20 09:10:00</td>\n",
       "      <td>1994-02-20 09:15:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.05</td>\n",
       "      <td>single_storm/ACME/1994/ACME_1994_02_3.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACME_199402.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1994-02-21 20:55:00</td>\n",
       "      <td>1994-02-22 00:00:00</td>\n",
       "      <td>185.0</td>\n",
       "      <td>18.54</td>\n",
       "      <td>15.24</td>\n",
       "      <td>single_storm/ACME/1994/ACME_1994_02_4.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACME_199402.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1994-02-22 00:05:00</td>\n",
       "      <td>1994-02-22 22:15:00</td>\n",
       "      <td>1330.0</td>\n",
       "      <td>29.97</td>\n",
       "      <td>12.19</td>\n",
       "      <td>single_storm/ACME/1994/ACME_1994_02_5.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              file station_id  event_id    year  month         event_start  \\\n",
       "0  ACME_199402.csv       ACME       1.0  1994.0    2.0 1994-02-18 17:30:00   \n",
       "1  ACME_199402.csv       ACME       2.0  1994.0    2.0 1994-02-19 18:25:00   \n",
       "2  ACME_199402.csv       ACME       3.0  1994.0    2.0 1994-02-20 09:10:00   \n",
       "3  ACME_199402.csv       ACME       4.0  1994.0    2.0 1994-02-21 20:55:00   \n",
       "4  ACME_199402.csv       ACME       5.0  1994.0    2.0 1994-02-22 00:05:00   \n",
       "\n",
       "            event_end  event_duration_min  event_total_depth_mm  \\\n",
       "0 1994-02-18 17:40:00                10.0                  2.54   \n",
       "1 1994-02-19 22:10:00               225.0                 10.16   \n",
       "2 1994-02-20 09:15:00                 5.0                  0.25   \n",
       "3 1994-02-22 00:00:00               185.0                 18.54   \n",
       "4 1994-02-22 22:15:00              1330.0                 29.97   \n",
       "\n",
       "   event_peak_intensity_mm_hr                                output_file note  \n",
       "0                       15.24  single_storm/ACME/1994/ACME_1994_02_1.csv  NaN  \n",
       "1                       51.82  single_storm/ACME/1994/ACME_1994_02_2.csv  NaN  \n",
       "2                        3.05  single_storm/ACME/1994/ACME_1994_02_3.csv  NaN  \n",
       "3                       15.24  single_storm/ACME/1994/ACME_1994_02_4.csv  NaN  \n",
       "4                       12.19  single_storm/ACME/1994/ACME_1994_02_5.csv  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, separate_storm_events\n",
    "importlib.reload(separate_storm_events)\n",
    "\n",
    "summary = separate_storm_events.separate_storm_events(\n",
    "    input_dir=\"storm_interval_information\",\n",
    "    output_dir=\"single_storm\",\n",
    "    recursive=True,\n",
    "    station_col=\"stid\",\n",
    "    cumulative_col=\"cumulative rain depth (mm)\",\n",
    "    columns_to_reset=None,   # or pass your own list (must match lowercase names)\n",
    "    sort_by_time_col=\"time\"\n",
    ")\n",
    "summary.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erosivity Storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>station_id</th>\n",
       "      <th>kept</th>\n",
       "      <th>reason</th>\n",
       "      <th>output_file</th>\n",
       "      <th>max_cumulative_mm</th>\n",
       "      <th>threshold_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME_1994_02_1.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>False</td>\n",
       "      <td>max(cumulative rain depth (mm))=2.54 &lt; thresho...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACME_1994_02_2.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>False</td>\n",
       "      <td>max(cumulative rain depth (mm))=10.16 &lt; thresh...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACME_1994_02_3.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>False</td>\n",
       "      <td>max(cumulative rain depth (mm))=0.254 &lt; thresh...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACME_1994_02_4.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>erosive_storms/ACME/1994/ACME_1994_02_4.csv</td>\n",
       "      <td>18.542</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACME_1994_02_5.csv</td>\n",
       "      <td>ACME</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>erosive_storms/ACME/1994/ACME_1994_02_5.csv</td>\n",
       "      <td>29.972</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file station_id   kept  \\\n",
       "0  ACME_1994_02_1.csv       ACME  False   \n",
       "1  ACME_1994_02_2.csv       ACME  False   \n",
       "2  ACME_1994_02_3.csv       ACME  False   \n",
       "3  ACME_1994_02_4.csv       ACME   True   \n",
       "4  ACME_1994_02_5.csv       ACME   True   \n",
       "\n",
       "                                              reason  \\\n",
       "0  max(cumulative rain depth (mm))=2.54 < thresho...   \n",
       "1  max(cumulative rain depth (mm))=10.16 < thresh...   \n",
       "2  max(cumulative rain depth (mm))=0.254 < thresh...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                   output_file  max_cumulative_mm  \\\n",
       "0                                         None                NaN   \n",
       "1                                         None                NaN   \n",
       "2                                         None                NaN   \n",
       "3  erosive_storms/ACME/1994/ACME_1994_02_4.csv             18.542   \n",
       "4  erosive_storms/ACME/1994/ACME_1994_02_5.csv             29.972   \n",
       "\n",
       "   threshold_mm  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3          12.7  \n",
       "4          12.7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, erosive_storms\n",
    "importlib.reload(erosive_storms)\n",
    "\n",
    "summary = erosive_storms.filter_erosive_storms(\n",
    "    input_dir=\"single_storm\",\n",
    "    output_dir=\"erosive_storms\",\n",
    "    threshold_mm=12.7,  # 12.7 mm (~0.5 in)\n",
    "    cumulative_col=\"cumulative rain depth (mm)\",  # must match your files\n",
    "    station_col=\"stid\",\n",
    "    time_col=\"time\"\n",
    ")\n",
    "summary.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rainfall erosivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
      "/Users/mengting.chen91/Desktop/Gauge Rainfall Erosivity/rainfall_erosivity.py:188: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stid</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>total rainfall for single event (mm)</th>\n",
       "      <th>rainfall @ max 30-min (mm)</th>\n",
       "      <th>max 30-min intensity (mm/h)</th>\n",
       "      <th>total energy (MJ/ha)</th>\n",
       "      <th>rainfall erosivity ((MJ-mm)/(ha-hr))</th>\n",
       "      <th>storm file</th>\n",
       "      <th>output_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1994</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>18.542</td>\n",
       "      <td>11.938</td>\n",
       "      <td>23.876</td>\n",
       "      <td>3.296564</td>\n",
       "      <td>78.708765</td>\n",
       "      <td>ACME_1994_02_4.csv</td>\n",
       "      <td>rainfall_erosivity/ACME/ACME_1994_02.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1994</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>29.972</td>\n",
       "      <td>21.336</td>\n",
       "      <td>42.672</td>\n",
       "      <td>4.869256</td>\n",
       "      <td>207.780905</td>\n",
       "      <td>ACME_1994_02_5.csv</td>\n",
       "      <td>rainfall_erosivity/ACME/ACME_1994_02.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1994</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>55.626</td>\n",
       "      <td>33.782</td>\n",
       "      <td>67.564</td>\n",
       "      <td>10.435005</td>\n",
       "      <td>705.030662</td>\n",
       "      <td>ACME_1994_03_9.csv</td>\n",
       "      <td>rainfall_erosivity/ACME/ACME_1994_03.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1994</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>13.462</td>\n",
       "      <td>9.906</td>\n",
       "      <td>19.812</td>\n",
       "      <td>3.172214</td>\n",
       "      <td>62.847910</td>\n",
       "      <td>ACME_1994_04_14.csv</td>\n",
       "      <td>rainfall_erosivity/ACME/ACME_1994_04.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1994</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>22.860</td>\n",
       "      <td>9.398</td>\n",
       "      <td>18.796</td>\n",
       "      <td>5.506361</td>\n",
       "      <td>103.497569</td>\n",
       "      <td>ACME_1994_04_15.csv</td>\n",
       "      <td>rainfall_erosivity/ACME/ACME_1994_04.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stid  year  month  day  total rainfall for single event (mm)  \\\n",
       "0  ACME  1994      2   21                                18.542   \n",
       "1  ACME  1994      2   22                                29.972   \n",
       "2  ACME  1994      3    8                                55.626   \n",
       "3  ACME  1994      4   11                                13.462   \n",
       "4  ACME  1994      4   24                                22.860   \n",
       "\n",
       "   rainfall @ max 30-min (mm)  max 30-min intensity (mm/h)  \\\n",
       "0                      11.938                       23.876   \n",
       "1                      21.336                       42.672   \n",
       "2                      33.782                       67.564   \n",
       "3                       9.906                       19.812   \n",
       "4                       9.398                       18.796   \n",
       "\n",
       "   total energy (MJ/ha)  rainfall erosivity ((MJ-mm)/(ha-hr))  \\\n",
       "0              3.296564                             78.708765   \n",
       "1              4.869256                            207.780905   \n",
       "2             10.435005                            705.030662   \n",
       "3              3.172214                             62.847910   \n",
       "4              5.506361                            103.497569   \n",
       "\n",
       "            storm file                               output_file  \n",
       "0   ACME_1994_02_4.csv  rainfall_erosivity/ACME/ACME_1994_02.csv  \n",
       "1   ACME_1994_02_5.csv  rainfall_erosivity/ACME/ACME_1994_02.csv  \n",
       "2   ACME_1994_03_9.csv  rainfall_erosivity/ACME/ACME_1994_03.csv  \n",
       "3  ACME_1994_04_14.csv  rainfall_erosivity/ACME/ACME_1994_04.csv  \n",
       "4  ACME_1994_04_15.csv  rainfall_erosivity/ACME/ACME_1994_04.csv  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, rainfall_erosivity\n",
    "importlib.reload(rainfall_erosivity)\n",
    "\n",
    "summary = rainfall_erosivity.process_rainfall_erosivity(\n",
    "    input_dir=\"erosive_storms\",\n",
    "    output_dir=\"rainfall_erosivity\",\n",
    "    recursive=True\n",
    ")\n",
    "summary.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Erosivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Monthly erosivity written under: monthly_erosivity\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import monthly_erosivity\n",
    "\n",
    "reload(monthly_erosivity)  # only needed if you edit the .py file\n",
    "monthly_erosivity.process_monthly_erosivity(\n",
    "    input_dir=\"rainfall_erosivity\",\n",
    "    output_dir=\"monthly_erosivity\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make publish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrainfalltools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example CSV from one station\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRain_Data_High_Quality/ACME/1994/ACME_199401.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/rainfalltools/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess_intervals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseparate_storm_events\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merosive_storms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrainfall_erosivity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import rainfalltools as rt\n",
    "\n",
    "# Example CSV from one station\n",
    "df = pd.read_csv(\"Rain_Data_High_Quality/ACME/1994/ACME_199401.csv\")\n",
    "\n",
    "# Call a function from the package\n",
    "storms = rt.identify_storms(df)\n",
    "\n",
    "# Preview\n",
    "print(storms.head())\n",
    "\n",
    "# Save result\n",
    "storms.to_csv(\"YUKO_storms.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
