{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc2df3c-e31e-43fb-9129-89ef8f35afa7",
   "metadata": {},
   "source": [
    "# 1. Rainfall Erosivity Factor Computation Using Kinetic Energy Equation\n",
    "\n",
    "This code is developed to calculate rainfall erosivity factor (R factor) using kinetic energy equation based on **5-minute interval precipitation gauge data during 1994-2023** requested from Oklahoma Mesonet (chartman@mesonet.org). The data in the \"RAIN\" column is **cumulative rainfall amount with unit of inch**, which also includes **frozen precipitation** measured at the time of thaw. Each Mesonet observation contains a running accumulation of rainfall since either 6 PM CST or 7 PM CDT. As each new evening begins, the accumulated rainfall is reset to zero. The missing data are indicated by -99 or -999. More information can be seen https://mesonet.org/about/instruments.\n",
    "\n",
    "The computation method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719b05a-a79f-4425-bf2a-7bafe8b11c35",
   "metadata": {},
   "source": [
    "## 1.1 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d429074-06c0-4e9c-8a57-fd810af4e0ed",
   "metadata": {},
   "source": [
    "### 1.1.1 Filter out the missing data and classify rainfall data based on site ID (STID)\n",
    "Since the missing data are indicated by -99 or -999, this code removes missing data by filtering out the data less than zero, maintaining the data greater and equal to zero.\n",
    "This code also classify the rainfall data by site ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80c9b30-4dc4-4fbc-9575-efa5816e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import arcpy\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4598dff3-1cc6-4cce-b123-f66d682cee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for station ACME to f:\\R factor Calculation\\Each Station\\ACME.csv\n",
      "Saved data for station ADAX to f:\\R factor Calculation\\Each Station\\ADAX.csv\n",
      "Saved data for station ALTU to f:\\R factor Calculation\\Each Station\\ALTU.csv\n",
      "Saved data for station ALV2 to f:\\R factor Calculation\\Each Station\\ALV2.csv\n",
      "Saved data for station ANT2 to f:\\R factor Calculation\\Each Station\\ANT2.csv\n",
      "Saved data for station APAC to f:\\R factor Calculation\\Each Station\\APAC.csv\n",
      "Saved data for station ARD2 to f:\\R factor Calculation\\Each Station\\ARD2.csv\n",
      "Saved data for station ARNE to f:\\R factor Calculation\\Each Station\\ARNE.csv\n",
      "Saved data for station BEAV to f:\\R factor Calculation\\Each Station\\BEAV.csv\n",
      "Saved data for station BBOW to f:\\R factor Calculation\\Each Station\\BBOW.csv\n",
      "Saved data for station BUFF to f:\\R factor Calculation\\Each Station\\BUFF.csv\n",
      "Saved data for station BURB to f:\\R factor Calculation\\Each Station\\BURB.csv\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 636. MiB for an array with shape (9, 9257763) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Process each CSV file\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_5min_rainfall, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Assuming the 'RAIN' column exists and filtering data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2144\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Hydro_process\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2298\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2300\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2301\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2302\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2304\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 636. MiB for an array with shape (9, 9257763) and data type int64"
     ]
    }
   ],
   "source": [
    "\n",
    "## Define the input directory for 5-minute rainfall data\n",
    "path_5min_rainfall = os.path.join(os.getcwd(), \"RAW\")\n",
    "\n",
    "# Define the output directory for saving storm data\n",
    "path_each_station = os.path.join(os.getcwd(), \"Each Station\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(path_each_station, exist_ok=True)\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in glob.glob(os.path.join(path_5min_rainfall, '*.csv')):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Assuming the 'RAIN' column exists and filtering data\n",
    "    df = df[df['RAIN'] >= 0]\n",
    "\n",
    "    # Extract unique station IDs\n",
    "    unique_stations = df['STID'].unique()\n",
    "\n",
    "    # Loop through each unique station, filter the DataFrame, and save to a new CSV\n",
    "    for station in unique_stations:\n",
    "        # Filter the DataFrame for the current station\n",
    "        df_station = df[df['STID'] == station][['STID', 'TIME', 'RAIN']]\n",
    "\n",
    "        # Define the output file path using the station name, safely handle file names\n",
    "        output_file_name = f\"{station.replace('/', '_')}.csv\"  # Replace '/' with '_' to avoid path issues\n",
    "        output_file_path = os.path.join(path_each_station, output_file_name)\n",
    "\n",
    "        # Save the filtered DataFrame to a CSV file\n",
    "        df_station.to_csv(output_file_path, index=False)\n",
    "        print(f\"Saved data for station {station} to {output_file_path}\")\n",
    "\n",
    "print(\"All files have been processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ca9ce-ab7d-48e5-87e3-a99a1b057666",
   "metadata": {},
   "source": [
    "### 1.1.2 Convert Time Format and Select Station \n",
    "The time format of the raw data from Oklahoma Mesonet is string format, which cannot be used to conduct calculations. \n",
    "The rainfall erosivity factor needs long-term measurement data, at least 10 years suggested by AH ARS (2013). Hence, this section:\n",
    "1) uses the **to_datetime function** to covert the time format for computation (more information about **to_datetime function** can be learned from: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html);\n",
    "2) filters out the stations with measurement periods of less than 10 years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf67135-19ac-4b65-abaf-9945040f3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved long-term data for ACME.csv\n",
      "Saved long-term data for ADAX.csv\n",
      "Saved long-term data for ALTU.csv\n",
      "Saved long-term data for ALV2.csv\n",
      "Saved long-term data for ANT2.csv\n",
      "Saved long-term data for APAC.csv\n",
      "Saved long-term data for ARD2.csv\n",
      "Saved long-term data for ARNE.csv\n",
      "Skipping BBOW.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for BEAV.csv\n",
      "Saved long-term data for BESS.csv\n",
      "Saved long-term data for BIXB.csv\n",
      "Saved long-term data for BLAC.csv\n",
      "Saved long-term data for BOIS.csv\n",
      "Saved long-term data for BREC.csv\n",
      "Saved long-term data for BRIS.csv\n",
      "Saved long-term data for BUFF.csv\n",
      "Saved long-term data for BURB.csv\n",
      "Saved long-term data for BURN.csv\n",
      "Saved long-term data for BUTL.csv\n",
      "Saved long-term data for BYAR.csv\n",
      "Saved long-term data for CAMA.csv\n",
      "Saved long-term data for CARL.csv\n",
      "Saved long-term data for CENT.csv\n",
      "Saved long-term data for CHAN.csv\n",
      "Saved long-term data for CHER.csv\n",
      "Saved long-term data for CHEY.csv\n",
      "Saved long-term data for CHIC.csv\n",
      "Saved long-term data for CLAY.csv\n",
      "Saved long-term data for CLOU.csv\n",
      "Saved long-term data for COOK.csv\n",
      "Saved long-term data for COPA.csv\n",
      "Saved long-term data for DURA.csv\n",
      "Skipping ELKC.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for ELRE.csv\n",
      "Saved long-term data for ERIC.csv\n",
      "Saved long-term data for EUFA.csv\n",
      "Skipping EVAX.csv as data recording period is less than 10 years.\n",
      "Skipping FAI2.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for FITT.csv\n",
      "Saved long-term data for FORA.csv\n",
      "Saved long-term data for FREE.csv\n",
      "Saved long-term data for FTCB.csv\n",
      "Saved long-term data for GOOD.csv\n",
      "Saved long-term data for GRA2.csv\n",
      "Saved long-term data for GUTH.csv\n",
      "Saved long-term data for HASK.csv\n",
      "Saved long-term data for HECT.csv\n",
      "Saved long-term data for HINT.csv\n",
      "Saved long-term data for HOBA.csv\n",
      "Saved long-term data for HOLD.csv\n",
      "Saved long-term data for HOLL.csv\n",
      "Saved long-term data for HOOK.csv\n",
      "Saved long-term data for HUGO.csv\n",
      "Saved long-term data for IDAB.csv\n",
      "Saved long-term data for INOL.csv\n",
      "Saved long-term data for JAYX.csv\n",
      "Saved long-term data for KENT.csv\n",
      "Saved long-term data for KETC.csv\n",
      "Saved long-term data for KIN2.csv\n",
      "Saved long-term data for LAHO.csv\n",
      "Saved long-term data for LANE.csv\n",
      "Saved long-term data for MADI.csv\n",
      "Saved long-term data for MANG.csv\n",
      "Saved long-term data for MARE.csv\n",
      "Saved long-term data for MAYR.csv\n",
      "Saved long-term data for MCAL.csv\n",
      "Saved long-term data for MEDF.csv\n",
      "Saved long-term data for MEDI.csv\n",
      "Saved long-term data for MIAM.csv\n",
      "Saved long-term data for MINC.csv\n",
      "Saved long-term data for MRSH.csv\n",
      "Saved long-term data for MTHE.csv\n",
      "Saved long-term data for NEWK.csv\n",
      "Saved long-term data for NEWP.csv\n",
      "Saved long-term data for NOWA.csv\n",
      "Saved long-term data for NRMN.csv\n",
      "Saved long-term data for OILT.csv\n",
      "Saved long-term data for OKCE.csv\n",
      "Saved long-term data for OKEM.csv\n",
      "Saved long-term data for OKMU.csv\n",
      "Saved long-term data for PAUL.csv\n",
      "Saved long-term data for PAWN.csv\n",
      "Saved long-term data for PERK.csv\n",
      "Saved long-term data for PORT.csv\n",
      "Saved long-term data for PRYO.csv\n",
      "Saved long-term data for PUTN.csv\n",
      "Saved long-term data for REDR.csv\n",
      "Saved long-term data for RING.csv\n",
      "Saved long-term data for SALL.csv\n",
      "Saved long-term data for SEIL.csv\n",
      "Skipping SEMI.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for SHAW.csv\n",
      "Saved long-term data for SKIA.csv\n",
      "Saved long-term data for SLAP.csv\n",
      "Saved long-term data for SPEN.csv\n",
      "Saved long-term data for STIG.csv\n",
      "Saved long-term data for STIL.csv\n",
      "Saved long-term data for STUA.csv\n",
      "Saved long-term data for SULP.csv\n",
      "Saved long-term data for TAHL.csv\n",
      "Skipping TALA.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for TALI.csv\n",
      "Saved long-term data for TIPT.csv\n",
      "Saved long-term data for TISH.csv\n",
      "Saved long-term data for TULN.csv\n",
      "Skipping VALL.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for VINI.csv\n",
      "Saved long-term data for WAL2.csv\n",
      "Saved long-term data for WASH.csv\n",
      "Saved long-term data for WATO.csv\n",
      "Saved long-term data for WAUR.csv\n",
      "Saved long-term data for WEAT.csv\n",
      "Skipping WEB3.csv as data recording period is less than 10 years.\n",
      "Saved long-term data for WEST.csv\n",
      "Saved long-term data for WILB.csv\n",
      "Saved long-term data for WIST.csv\n",
      "Saved long-term data for WOOD.csv\n",
      "Saved long-term data for WYNO.csv\n",
      "Skipping YUKO.csv as data recording period is less than 10 years.\n"
     ]
    }
   ],
   "source": [
    "# Input and output directories\n",
    "path_each_station = os.path.join(os.getcwd(), \"Each Station\")\n",
    "\n",
    "path_long_term = os.path.join(os.getcwd(), \"Long-term Stations\")\n",
    "os.makedirs(path_long_term, exist_ok=True)\n",
    "\n",
    "path_result = os.path.join(os.getcwd(), \"Result\")\n",
    "os.makedirs(path_result, exist_ok=True)\n",
    "\n",
    "# Initialize an empty list to store station-wise data recording periods\n",
    "station_periods = []\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for filename in os.listdir(path_each_station):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(path_each_station, filename)\n",
    "\n",
    "        # Read the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Convert the value in \"TIME\" column from string format to datetime objects\n",
    "        df[\"TIME\"] = pd.to_datetime(df[\"TIME\"], format=\"%Y-%m-%dT%H:%M\", errors='coerce')\n",
    "\n",
    "        # Calculate the measurement period for the station\n",
    "        period = df[\"TIME\"].max() - df[\"TIME\"].min()  # Get the timedelta object\n",
    "        \n",
    "        # Calculate period in years\n",
    "        period_years = period.days / 365.25  # Considering leap years\n",
    "        \n",
    "        # Append station and its period to the list\n",
    "        station_periods.append({\n",
    "            'STID': filename.split('.')[0], \n",
    "            'Start Time': df[\"TIME\"].min(), \n",
    "            'End Time': df[\"TIME\"].max(), \n",
    "            'Period': period,\n",
    "            'Period Years': period_years\n",
    "        })\n",
    "\n",
    "        # If the period is less than 10 years, skip processing and move to the next station\n",
    "        if period.days < 365 * 10:  # Check if the recording period is less than 10 years\n",
    "            print(f\"Skipping {filename} as data recording period is less than 10 years.\")\n",
    "            continue\n",
    "\n",
    "        # Define the output path for long-term stations\n",
    "        new_file_path = os.path.join(path_long_term, filename)\n",
    "\n",
    "        # Save the filtered DataFrame to the new CSV file\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        print(f\"Saved long-term data for {filename}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame for recording periods\n",
    "station_periods_df = pd.DataFrame(station_periods)\n",
    "\n",
    "# Save the station-wise data recording periods DataFrame to a CSV file\n",
    "station_periods_df.to_csv(os.path.join(path_result, 'station_periods.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79dbc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ACME.csv\n",
      "Measurement period for ACME.csv: 29.95208761122519 years\n",
      "Saved long-term data for ACME.csv\n",
      "Processing file: ADAX.csv\n",
      "Measurement period for ADAX.csv: 30.083504449007528 years\n",
      "Saved long-term data for ADAX.csv\n",
      "Processing file: ALTU.csv\n",
      "Measurement period for ALTU.csv: 30.083504449007528 years\n",
      "Saved long-term data for ALTU.csv\n",
      "Processing file: ALV2.csv\n",
      "Measurement period for ALV2.csv: 25.12251882272416 years\n",
      "Processing file: ANT2.csv\n",
      "Measurement period for ANT2.csv: 12.799452429842573 years\n",
      "Processing file: APAC.csv\n",
      "Measurement period for APAC.csv: 29.9958932238193 years\n",
      "Saved long-term data for APAC.csv\n",
      "Processing file: ARD2.csv\n",
      "Measurement period for ARD2.csv: 19.939767282683093 years\n",
      "Processing file: ARNE.csv\n",
      "Measurement period for ARNE.csv: 30.083504449007528 years\n",
      "Saved long-term data for ARNE.csv\n",
      "Processing file: BBOW.csv\n",
      "Measurement period for BBOW.csv: 8.131416837782341 years\n",
      "Processing file: BEAV.csv\n",
      "Measurement period for BEAV.csv: 30.083504449007528 years\n",
      "Saved long-term data for BEAV.csv\n",
      "Processing file: BESS.csv\n",
      "Measurement period for BESS.csv: 29.336071184120467 years\n",
      "Saved long-term data for BESS.csv\n",
      "Processing file: BIXB.csv\n",
      "Measurement period for BIXB.csv: 29.322381930184804 years\n",
      "Saved long-term data for BIXB.csv\n",
      "Processing file: BLAC.csv\n",
      "Measurement period for BLAC.csv: 29.336071184120467 years\n",
      "Saved long-term data for BLAC.csv\n",
      "Processing file: BOIS.csv\n",
      "Measurement period for BOIS.csv: 29.508555783709788 years\n",
      "Saved long-term data for BOIS.csv\n",
      "Processing file: BREC.csv\n",
      "Measurement period for BREC.csv: 30.083504449007528 years\n",
      "Saved long-term data for BREC.csv\n",
      "Processing file: BRIS.csv\n",
      "Measurement period for BRIS.csv: 30.083504449007528 years\n",
      "Saved long-term data for BRIS.csv\n",
      "Processing file: BUFF.csv\n",
      "Measurement period for BUFF.csv: 29.336071184120467 years\n",
      "Saved long-term data for BUFF.csv\n",
      "Processing file: BURB.csv\n",
      "Measurement period for BURB.csv: 29.336071184120467 years\n",
      "Saved long-term data for BURB.csv\n",
      "Processing file: BURN.csv\n",
      "Measurement period for BURN.csv: 30.083504449007528 years\n",
      "Saved long-term data for BURN.csv\n",
      "Processing file: BUTL.csv\n",
      "Measurement period for BUTL.csv: 30.083504449007528 years\n",
      "Saved long-term data for BUTL.csv\n",
      "Processing file: BYAR.csv\n",
      "Measurement period for BYAR.csv: 30.083504449007528 years\n",
      "Saved long-term data for BYAR.csv\n",
      "Processing file: CAMA.csv\n",
      "Measurement period for CAMA.csv: 29.336071184120467 years\n",
      "Saved long-term data for CAMA.csv\n",
      "Processing file: CARL.csv\n",
      "Measurement period for CARL.csv: 15.917864476386036 years\n",
      "Processing file: CENT.csv\n",
      "Measurement period for CENT.csv: 29.336071184120467 years\n",
      "Saved long-term data for CENT.csv\n",
      "Processing file: CHAN.csv\n",
      "Measurement period for CHAN.csv: 30.083504449007528 years\n",
      "Saved long-term data for CHAN.csv\n",
      "Processing file: CHER.csv\n",
      "Measurement period for CHER.csv: 30.083504449007528 years\n",
      "Saved long-term data for CHER.csv\n",
      "Processing file: CHEY.csv\n",
      "Measurement period for CHEY.csv: 30.083504449007528 years\n",
      "Saved long-term data for CHEY.csv\n",
      "Processing file: CHIC.csv\n",
      "Measurement period for CHIC.csv: 30.083504449007528 years\n",
      "Saved long-term data for CHIC.csv\n",
      "Processing file: CLAY.csv\n",
      "Measurement period for CLAY.csv: 30.083504449007528 years\n",
      "Saved long-term data for CLAY.csv\n",
      "Processing file: CLOU.csv\n",
      "Measurement period for CLOU.csv: 30.083504449007528 years\n",
      "Saved long-term data for CLOU.csv\n",
      "Processing file: COOK.csv\n",
      "Measurement period for COOK.csv: 30.083504449007528 years\n",
      "Saved long-term data for COOK.csv\n",
      "Processing file: COPA.csv\n",
      "Measurement period for COPA.csv: 30.083504449007528 years\n",
      "Saved long-term data for COPA.csv\n",
      "Processing file: DURA.csv\n",
      "Measurement period for DURA.csv: 30.083504449007528 years\n",
      "Saved long-term data for DURA.csv\n",
      "Processing file: ELKC.csv\n",
      "Measurement period for ELKC.csv: 8.640657084188911 years\n",
      "Processing file: ELRE.csv\n",
      "Measurement period for ELRE.csv: 29.336071184120467 years\n",
      "Saved long-term data for ELRE.csv\n",
      "Processing file: ERIC.csv\n",
      "Measurement period for ERIC.csv: 29.336071184120467 years\n",
      "Saved long-term data for ERIC.csv\n",
      "Processing file: EUFA.csv\n",
      "Measurement period for EUFA.csv: 30.083504449007528 years\n",
      "Saved long-term data for EUFA.csv\n",
      "Processing file: EVAX.csv\n",
      "Measurement period for EVAX.csv: 7.704312114989733 years\n",
      "Processing file: FAI2.csv\n",
      "Measurement period for FAI2.csv: 0.2108145106091718 years\n",
      "Processing file: FITT.csv\n",
      "Measurement period for FITT.csv: 18.72142368240931 years\n",
      "Processing file: FORA.csv\n",
      "Measurement period for FORA.csv: 30.083504449007528 years\n",
      "Saved long-term data for FORA.csv\n",
      "Processing file: FREE.csv\n",
      "Measurement period for FREE.csv: 30.083504449007528 years\n",
      "Saved long-term data for FREE.csv\n",
      "Processing file: FTCB.csv\n",
      "Measurement period for FTCB.csv: 30.083504449007528 years\n",
      "Saved long-term data for FTCB.csv\n",
      "Processing file: GOOD.csv\n",
      "Measurement period for GOOD.csv: 28.539356605065024 years\n",
      "Saved long-term data for GOOD.csv\n",
      "Processing file: GRA2.csv\n",
      "Measurement period for GRA2.csv: 24.83504449007529 years\n",
      "Processing file: GUTH.csv\n",
      "Measurement period for GUTH.csv: 29.336071184120467 years\n",
      "Saved long-term data for GUTH.csv\n",
      "Processing file: HASK.csv\n",
      "Measurement period for HASK.csv: 29.336071184120467 years\n",
      "Saved long-term data for HASK.csv\n",
      "Processing file: HECT.csv\n",
      "Measurement period for HECT.csv: 27.59479808350445 years\n",
      "Saved long-term data for HECT.csv\n",
      "Processing file: HINT.csv\n",
      "Measurement period for HINT.csv: 30.083504449007528 years\n",
      "Saved long-term data for HINT.csv\n",
      "Processing file: HOBA.csv\n",
      "Measurement period for HOBA.csv: 30.083504449007528 years\n",
      "Saved long-term data for HOBA.csv\n",
      "Processing file: HOLD.csv\n",
      "Measurement period for HOLD.csv: 14.674880219028063 years\n",
      "Processing file: HOLL.csv\n",
      "Measurement period for HOLL.csv: 30.083504449007528 years\n",
      "Saved long-term data for HOLL.csv\n",
      "Processing file: HOOK.csv\n",
      "Measurement period for HOOK.csv: 30.083504449007528 years\n",
      "Saved long-term data for HOOK.csv\n",
      "Processing file: HUGO.csv\n",
      "Measurement period for HUGO.csv: 30.083504449007528 years\n",
      "Saved long-term data for HUGO.csv\n",
      "Processing file: IDAB.csv\n",
      "Measurement period for IDAB.csv: 30.083504449007528 years\n",
      "Saved long-term data for IDAB.csv\n",
      "Processing file: INOL.csv\n",
      "Measurement period for INOL.csv: 22.168377823408623 years\n",
      "Processing file: JAYX.csv\n",
      "Measurement period for JAYX.csv: 29.68104038329911 years\n",
      "Saved long-term data for JAYX.csv\n",
      "Processing file: KENT.csv\n",
      "Measurement period for KENT.csv: 29.336071184120467 years\n",
      "Saved long-term data for KENT.csv\n",
      "Processing file: KETC.csv\n",
      "Measurement period for KETC.csv: 29.336071184120467 years\n",
      "Saved long-term data for KETC.csv\n",
      "Processing file: KIN2.csv\n",
      "Measurement period for KIN2.csv: 14.904859685147159 years\n",
      "Processing file: LAHO.csv\n",
      "Measurement period for LAHO.csv: 30.083504449007528 years\n",
      "Saved long-term data for LAHO.csv\n",
      "Processing file: LANE.csv\n",
      "Measurement period for LANE.csv: 30.083504449007528 years\n",
      "Saved long-term data for LANE.csv\n",
      "Processing file: MADI.csv\n",
      "Measurement period for MADI.csv: 30.083504449007528 years\n",
      "Saved long-term data for MADI.csv\n",
      "Processing file: MANG.csv\n",
      "Measurement period for MANG.csv: 30.083504449007528 years\n",
      "Saved long-term data for MANG.csv\n",
      "Processing file: MARE.csv\n",
      "Measurement period for MARE.csv: 30.083504449007528 years\n",
      "Saved long-term data for MARE.csv\n",
      "Processing file: MAYR.csv\n",
      "Measurement period for MAYR.csv: 30.083504449007528 years\n",
      "Saved long-term data for MAYR.csv\n",
      "Processing file: MCAL.csv\n",
      "Measurement period for MCAL.csv: 30.083504449007528 years\n",
      "Saved long-term data for MCAL.csv\n",
      "Processing file: MEDF.csv\n",
      "Measurement period for MEDF.csv: 29.741273100616016 years\n",
      "Saved long-term data for MEDF.csv\n",
      "Processing file: MEDI.csv\n",
      "Measurement period for MEDI.csv: 29.336071184120467 years\n",
      "Saved long-term data for MEDI.csv\n",
      "Processing file: MIAM.csv\n",
      "Measurement period for MIAM.csv: 29.336071184120467 years\n",
      "Saved long-term data for MIAM.csv\n",
      "Processing file: MINC.csv\n",
      "Measurement period for MINC.csv: 29.336071184120467 years\n",
      "Saved long-term data for MINC.csv\n",
      "Processing file: MRSH.csv\n",
      "Measurement period for MRSH.csv: 20.35044490075291 years\n",
      "Processing file: MTHE.csv\n",
      "Measurement period for MTHE.csv: 30.083504449007528 years\n",
      "Saved long-term data for MTHE.csv\n",
      "Processing file: NEWK.csv\n",
      "Measurement period for NEWK.csv: 30.083504449007528 years\n",
      "Saved long-term data for NEWK.csv\n",
      "Processing file: NEWP.csv\n",
      "Measurement period for NEWP.csv: 21.32785763175907 years\n",
      "Processing file: NOWA.csv\n",
      "Measurement period for NOWA.csv: 30.083504449007528 years\n",
      "Saved long-term data for NOWA.csv\n",
      "Processing file: NRMN.csv\n",
      "Measurement period for NRMN.csv: 21.587953456536617 years\n",
      "Processing file: OILT.csv\n",
      "Measurement period for OILT.csv: 30.083504449007528 years\n",
      "Saved long-term data for OILT.csv\n",
      "Processing file: OKCE.csv\n",
      "Measurement period for OKCE.csv: 16.78302532511978 years\n",
      "Processing file: OKEM.csv\n",
      "Measurement period for OKEM.csv: 29.61259411362081 years\n",
      "Saved long-term data for OKEM.csv\n",
      "Processing file: OKMU.csv\n",
      "Measurement period for OKMU.csv: 30.083504449007528 years\n",
      "Saved long-term data for OKMU.csv\n",
      "Processing file: PAUL.csv\n",
      "Measurement period for PAUL.csv: 30.012320328542096 years\n",
      "Saved long-term data for PAUL.csv\n",
      "Processing file: PAWN.csv\n",
      "Measurement period for PAWN.csv: 30.083504449007528 years\n",
      "Saved long-term data for PAWN.csv\n",
      "Processing file: PERK.csv\n",
      "Measurement period for PERK.csv: 30.083504449007528 years\n",
      "Saved long-term data for PERK.csv\n",
      "Processing file: PORT.csv\n",
      "Measurement period for PORT.csv: 24.23545516769336 years\n",
      "Processing file: PRYO.csv\n",
      "Measurement period for PRYO.csv: 30.083504449007528 years\n",
      "Saved long-term data for PRYO.csv\n",
      "Processing file: PUTN.csv\n",
      "Measurement period for PUTN.csv: 30.083504449007528 years\n",
      "Saved long-term data for PUTN.csv\n",
      "Processing file: REDR.csv\n",
      "Measurement period for REDR.csv: 30.083504449007528 years\n",
      "Saved long-term data for REDR.csv\n",
      "Processing file: RING.csv\n",
      "Measurement period for RING.csv: 28.865160848733744 years\n",
      "Saved long-term data for RING.csv\n",
      "Processing file: SALL.csv\n",
      "Measurement period for SALL.csv: 30.083504449007528 years\n",
      "Saved long-term data for SALL.csv\n",
      "Processing file: SEIL.csv\n",
      "Measurement period for SEIL.csv: 29.336071184120467 years\n",
      "Saved long-term data for SEIL.csv\n",
      "Processing file: SEMI.csv\n",
      "Measurement period for SEMI.csv: 5.25119780971937 years\n",
      "Processing file: SHAW.csv\n",
      "Measurement period for SHAW.csv: 29.336071184120467 years\n",
      "Saved long-term data for SHAW.csv\n",
      "Processing file: SKIA.csv\n",
      "Measurement period for SKIA.csv: 30.083504449007528 years\n",
      "Saved long-term data for SKIA.csv\n",
      "Processing file: SLAP.csv\n",
      "Measurement period for SLAP.csv: 30.083504449007528 years\n",
      "Saved long-term data for SLAP.csv\n",
      "Processing file: SPEN.csv\n",
      "Measurement period for SPEN.csv: 30.083504449007528 years\n",
      "Saved long-term data for SPEN.csv\n",
      "Processing file: STIG.csv\n",
      "Measurement period for STIG.csv: 30.083504449007528 years\n",
      "Saved long-term data for STIG.csv\n",
      "Processing file: STIL.csv\n",
      "Measurement period for STIL.csv: 30.083504449007528 years\n",
      "Saved long-term data for STIL.csv\n",
      "Processing file: STUA.csv\n",
      "Measurement period for STUA.csv: 30.083504449007528 years\n",
      "Saved long-term data for STUA.csv\n",
      "Processing file: SULP.csv\n",
      "Measurement period for SULP.csv: 30.083504449007528 years\n",
      "Saved long-term data for SULP.csv\n",
      "Processing file: TAHL.csv\n",
      "Measurement period for TAHL.csv: 30.083504449007528 years\n",
      "Saved long-term data for TAHL.csv\n",
      "Processing file: TALA.csv\n",
      "Measurement period for TALA.csv: 9.333333333333334 years\n",
      "Processing file: TALI.csv\n",
      "Measurement period for TALI.csv: 29.018480492813143 years\n",
      "Saved long-term data for TALI.csv\n",
      "Processing file: TIPT.csv\n",
      "Measurement period for TIPT.csv: 30.083504449007528 years\n",
      "Saved long-term data for TIPT.csv\n",
      "Processing file: TISH.csv\n",
      "Measurement period for TISH.csv: 30.083504449007528 years\n",
      "Saved long-term data for TISH.csv\n",
      "Processing file: TULN.csv\n",
      "Measurement period for TULN.csv: 10.598220396988363 years\n",
      "Processing file: VALL.csv\n",
      "Measurement period for VALL.csv: 8.2984257357974 years\n",
      "Processing file: VINI.csv\n",
      "Measurement period for VINI.csv: 29.336071184120467 years\n",
      "Saved long-term data for VINI.csv\n",
      "Processing file: WAL2.csv\n",
      "Measurement period for WAL2.csv: 11.88227241615332 years\n",
      "Processing file: WASH.csv\n",
      "Measurement period for WASH.csv: 29.842573579739906 years\n",
      "Saved long-term data for WASH.csv\n",
      "Processing file: WATO.csv\n",
      "Measurement period for WATO.csv: 30.083504449007528 years\n",
      "Saved long-term data for WATO.csv\n",
      "Processing file: WAUR.csv\n",
      "Measurement period for WAUR.csv: 30.083504449007528 years\n",
      "Saved long-term data for WAUR.csv\n",
      "Processing file: WEAT.csv\n",
      "Measurement period for WEAT.csv: 29.960301163586585 years\n",
      "Saved long-term data for WEAT.csv\n",
      "Processing file: WEB3.csv\n",
      "Measurement period for WEB3.csv: 2.625598904859685 years\n",
      "Processing file: WEST.csv\n",
      "Measurement period for WEST.csv: 30.083504449007528 years\n",
      "Saved long-term data for WEST.csv\n",
      "Processing file: WILB.csv\n",
      "Measurement period for WILB.csv: 30.083504449007528 years\n",
      "Saved long-term data for WILB.csv\n",
      "Processing file: WIST.csv\n",
      "Measurement period for WIST.csv: 30.083504449007528 years\n",
      "Saved long-term data for WIST.csv\n",
      "Processing file: WOOD.csv\n",
      "Measurement period for WOOD.csv: 30.083504449007528 years\n",
      "Saved long-term data for WOOD.csv\n",
      "Processing file: WYNO.csv\n",
      "Measurement period for WYNO.csv: 30.083504449007528 years\n",
      "Saved long-term data for WYNO.csv\n",
      "Processing file: YUKO.csv\n",
      "Measurement period for YUKO.csv: 5.620807665982204 years\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "path_each_station = os.path.join(os.getcwd(), \"Each Station\")\n",
    "path_26_year = os.path.join(os.getcwd(), \"26-year Stations\")\n",
    "os.makedirs(path_26_year, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for filename in os.listdir(path_each_station):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(path_each_station, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "\n",
    "        # Read the CSV file into a Pandas DataFrame\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check if 'TIME' column exists\n",
    "        if \"TIME\" not in df.columns:\n",
    "            print(f\"'TIME' column missing in {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Convert the 'TIME' column to datetime\n",
    "        df[\"TIME\"] = pd.to_datetime(df[\"TIME\"], format=\"%Y-%m-%dT%H:%M\", errors='coerce')\n",
    "        df = df.dropna(subset=[\"TIME\"])  # Remove invalid timestamps\n",
    "\n",
    "        # Check if the file has sufficient data for period calculation\n",
    "        if df[\"TIME\"].min() is pd.NaT or df[\"TIME\"].max() is pd.NaT:\n",
    "            print(f\"Invalid or insufficient 'TIME' data in {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the measurement period in years\n",
    "        period = df[\"TIME\"].max() - df[\"TIME\"].min()\n",
    "        period_years = period.days / 365.25\n",
    "        print(f\"Measurement period for {filename}: {period_years} years\")\n",
    "\n",
    "        # Check if the period is greater than or equal to 26 years\n",
    "        if period_years >= 26:\n",
    "            new_file_path = os.path.join(path_26_year, filename)\n",
    "            try:\n",
    "                df.to_csv(new_file_path, index=False)\n",
    "                print(f\"Saved long-term data for {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3fcdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file with required date range: ACME.csv\n",
      "Saved file with required date range: ADAX.csv\n",
      "Saved file with required date range: ALTU.csv\n",
      "Saved file with required date range: APAC.csv\n",
      "Saved file with required date range: ARNE.csv\n",
      "Saved file with required date range: BEAV.csv\n",
      "Saved file with required date range: BESS.csv\n",
      "Saved file with required date range: BIXB.csv\n",
      "Saved file with required date range: BLAC.csv\n",
      "Saved file with required date range: BOIS.csv\n",
      "Saved file with required date range: BREC.csv\n",
      "Saved file with required date range: BRIS.csv\n",
      "Saved file with required date range: BUFF.csv\n",
      "Saved file with required date range: BURB.csv\n",
      "Saved file with required date range: BURN.csv\n",
      "Saved file with required date range: BUTL.csv\n",
      "Saved file with required date range: BYAR.csv\n",
      "Saved file with required date range: CAMA.csv\n",
      "Saved file with required date range: CENT.csv\n",
      "Saved file with required date range: CHAN.csv\n",
      "Saved file with required date range: CHER.csv\n",
      "Saved file with required date range: CHEY.csv\n",
      "Saved file with required date range: CHIC.csv\n",
      "Saved file with required date range: CLAY.csv\n",
      "Saved file with required date range: CLOU.csv\n",
      "Saved file with required date range: COOK.csv\n",
      "Saved file with required date range: COPA.csv\n",
      "Saved file with required date range: DURA.csv\n",
      "Saved file with required date range: ELRE.csv\n",
      "Saved file with required date range: ERIC.csv\n",
      "Saved file with required date range: EUFA.csv\n",
      "Saved file with required date range: FORA.csv\n",
      "Saved file with required date range: FREE.csv\n",
      "Saved file with required date range: FTCB.csv\n",
      "Saved file with required date range: GUTH.csv\n",
      "Saved file with required date range: HASK.csv\n",
      "Saved file with required date range: HINT.csv\n",
      "Saved file with required date range: HOBA.csv\n",
      "Saved file with required date range: HOLL.csv\n",
      "Saved file with required date range: HOOK.csv\n",
      "Saved file with required date range: HUGO.csv\n",
      "Saved file with required date range: IDAB.csv\n",
      "Saved file with required date range: JAYX.csv\n",
      "Saved file with required date range: KENT.csv\n",
      "Saved file with required date range: KETC.csv\n",
      "Saved file with required date range: LAHO.csv\n",
      "Saved file with required date range: LANE.csv\n",
      "Saved file with required date range: MADI.csv\n",
      "Saved file with required date range: MARE.csv\n",
      "Saved file with required date range: MAYR.csv\n",
      "Saved file with required date range: MCAL.csv\n",
      "Saved file with required date range: MEDF.csv\n",
      "Saved file with required date range: MEDI.csv\n",
      "Saved file with required date range: MIAM.csv\n",
      "Saved file with required date range: MINC.csv\n",
      "Saved file with required date range: MTHE.csv\n",
      "Saved file with required date range: NEWK.csv\n",
      "Saved file with required date range: NOWA.csv\n",
      "Saved file with required date range: OILT.csv\n",
      "Saved file with required date range: OKEM.csv\n",
      "Saved file with required date range: OKMU.csv\n",
      "Saved file with required date range: PAUL.csv\n",
      "Saved file with required date range: PAWN.csv\n",
      "Saved file with required date range: PERK.csv\n",
      "Saved file with required date range: PRYO.csv\n",
      "Saved file with required date range: PUTN.csv\n",
      "Saved file with required date range: REDR.csv\n",
      "Saved file with required date range: SALL.csv\n",
      "Saved file with required date range: SEIL.csv\n",
      "Saved file with required date range: SHAW.csv\n",
      "Saved file with required date range: SKIA.csv\n",
      "Saved file with required date range: SLAP.csv\n",
      "Saved file with required date range: SPEN.csv\n",
      "Saved file with required date range: STIG.csv\n",
      "Saved file with required date range: STIL.csv\n",
      "Saved file with required date range: STUA.csv\n",
      "Saved file with required date range: SULP.csv\n",
      "Saved file with required date range: TAHL.csv\n",
      "Saved file with required date range: TIPT.csv\n",
      "Saved file with required date range: TISH.csv\n",
      "Saved file with required date range: VINI.csv\n",
      "Saved file with required date range: WASH.csv\n",
      "Saved file with required date range: WATO.csv\n",
      "Saved file with required date range: WAUR.csv\n",
      "Saved file with required date range: WEAT.csv\n",
      "Saved file with required date range: WEST.csv\n",
      "Saved file with required date range: WILB.csv\n",
      "Saved file with required date range: WIST.csv\n",
      "Saved file with required date range: WOOD.csv\n",
      "Saved file with required date range: WYNO.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "path_each_station = os.path.join(os.getcwd(), \"Each Station\")\n",
    "path_filtered = os.path.join(os.getcwd(), \"Filtered Stations\")  # Output folder\n",
    "os.makedirs(path_filtered, exist_ok=True)\n",
    "\n",
    "# Define the required date range\n",
    "start_date = pd.Timestamp(\"1995-01-01\")\n",
    "end_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for filename in os.listdir(path_each_station):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(path_each_station, filename)\n",
    "\n",
    "        # Read the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Convert the value in \"TIME\" column from string format to datetime objects\n",
    "        df[\"TIME\"] = pd.to_datetime(df[\"TIME\"], format=\"%Y-%m-%dT%H:%M\", errors='coerce')\n",
    "\n",
    "        # Filter out rows with invalid or missing dates\n",
    "        df = df.dropna(subset=[\"TIME\"])\n",
    "\n",
    "        # Filter the data to include only rows within the specified date range\n",
    "        df_filtered = df[(df[\"TIME\"] >= start_date) & (df[\"TIME\"] <= end_date)]\n",
    "\n",
    "        # Check if the filtered data covers the required date range\n",
    "        if not df_filtered.empty and df_filtered[\"TIME\"].min() <= start_date and df_filtered[\"TIME\"].max() >= end_date:\n",
    "            new_file_path = os.path.join(path_filtered, filename)\n",
    "            df_filtered.to_csv(new_file_path, index=False)\n",
    "            print(f\"Saved file with required date range: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594eb66-09ed-46c0-902f-899923cfb756",
   "metadata": {},
   "source": [
    "### 1.1.3 Obtain 5-minute interval rainfall depth\n",
    "Mesonet sites measured the accumulated rainfall at every 5 minutes. The column of \"RAIN\" in raw data requested from Mesonet is accumulated rainfall depth with unit inch.\n",
    "This section is to calculate rainfall depth for each 5-minute interval and save the results with unit millimeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cd8370-c693-4258-a936-a1a614ad2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storm data saved for ACME.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\ACME.csv\n",
      "Storm data saved for ADAX.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\ADAX.csv\n",
      "Storm data saved for ALTU.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\ALTU.csv\n",
      "Storm data saved for APAC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\APAC.csv\n",
      "Storm data saved for ARNE.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\ARNE.csv\n",
      "Storm data saved for BEAV.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BEAV.csv\n",
      "Storm data saved for BESS.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BESS.csv\n",
      "Storm data saved for BIXB.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BIXB.csv\n",
      "Storm data saved for BLAC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BLAC.csv\n",
      "Storm data saved for BOIS.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BOIS.csv\n",
      "Storm data saved for BREC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BREC.csv\n",
      "Storm data saved for BRIS.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BRIS.csv\n",
      "Storm data saved for BUFF.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BUFF.csv\n",
      "Storm data saved for BURB.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BURB.csv\n",
      "Storm data saved for BURN.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BURN.csv\n",
      "Storm data saved for BUTL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BUTL.csv\n",
      "Storm data saved for BYAR.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\BYAR.csv\n",
      "Storm data saved for CAMA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CAMA.csv\n",
      "Storm data saved for CENT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CENT.csv\n",
      "Storm data saved for CHAN.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CHAN.csv\n",
      "Storm data saved for CHER.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CHER.csv\n",
      "Storm data saved for CHEY.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CHEY.csv\n",
      "Storm data saved for CHIC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CHIC.csv\n",
      "Storm data saved for CLAY.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CLAY.csv\n",
      "Storm data saved for CLOU.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\CLOU.csv\n",
      "Storm data saved for COOK.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\COOK.csv\n",
      "Storm data saved for COPA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\COPA.csv\n",
      "Storm data saved for DURA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\DURA.csv\n",
      "Storm data saved for ELRE.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\ELRE.csv\n",
      "Storm data saved for ERIC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\ERIC.csv\n",
      "Storm data saved for EUFA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\EUFA.csv\n",
      "Storm data saved for FORA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\FORA.csv\n",
      "Storm data saved for FREE.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\FREE.csv\n",
      "Storm data saved for FTCB.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\FTCB.csv\n",
      "Storm data saved for GOOD.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\GOOD.csv\n",
      "Storm data saved for GUTH.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\GUTH.csv\n",
      "Storm data saved for HASK.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HASK.csv\n",
      "Storm data saved for HECT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HECT.csv\n",
      "Storm data saved for HINT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HINT.csv\n",
      "Storm data saved for HOBA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HOBA.csv\n",
      "Storm data saved for HOLL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HOLL.csv\n",
      "Storm data saved for HOOK.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HOOK.csv\n",
      "Storm data saved for HUGO.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\HUGO.csv\n",
      "Storm data saved for IDAB.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\IDAB.csv\n",
      "Storm data saved for JAYX.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\JAYX.csv\n",
      "Storm data saved for KENT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\KENT.csv\n",
      "Storm data saved for KETC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\KETC.csv\n",
      "Storm data saved for LAHO.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\LAHO.csv\n",
      "Storm data saved for LANE.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\LANE.csv\n",
      "Storm data saved for MADI.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MADI.csv\n",
      "Storm data saved for MANG.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MANG.csv\n",
      "Storm data saved for MARE.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MARE.csv\n",
      "Storm data saved for MAYR.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MAYR.csv\n",
      "Storm data saved for MCAL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MCAL.csv\n",
      "Storm data saved for MEDF.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MEDF.csv\n",
      "Storm data saved for MEDI.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MEDI.csv\n",
      "Storm data saved for MIAM.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MIAM.csv\n",
      "Storm data saved for MINC.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MINC.csv\n",
      "Storm data saved for MTHE.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\MTHE.csv\n",
      "Storm data saved for NEWK.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\NEWK.csv\n",
      "Storm data saved for NOWA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\NOWA.csv\n",
      "Storm data saved for OILT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\OILT.csv\n",
      "Storm data saved for OKEM.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\OKEM.csv\n",
      "Storm data saved for OKMU.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\OKMU.csv\n",
      "Storm data saved for PAUL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\PAUL.csv\n",
      "Storm data saved for PAWN.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\PAWN.csv\n",
      "Storm data saved for PERK.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\PERK.csv\n",
      "Storm data saved for PRYO.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\PRYO.csv\n",
      "Storm data saved for PUTN.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\PUTN.csv\n",
      "Storm data saved for REDR.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\REDR.csv\n",
      "Storm data saved for RING.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\RING.csv\n",
      "Storm data saved for SALL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SALL.csv\n",
      "Storm data saved for SEIL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SEIL.csv\n",
      "Storm data saved for SHAW.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SHAW.csv\n",
      "Storm data saved for SKIA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SKIA.csv\n",
      "Storm data saved for SLAP.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SLAP.csv\n",
      "Storm data saved for SPEN.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SPEN.csv\n",
      "Storm data saved for STIG.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\STIG.csv\n",
      "Storm data saved for STIL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\STIL.csv\n",
      "Storm data saved for STUA.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\STUA.csv\n",
      "Storm data saved for SULP.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\SULP.csv\n",
      "Storm data saved for TAHL.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\TAHL.csv\n",
      "Storm data saved for TALI.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\TALI.csv\n",
      "Storm data saved for TIPT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\TIPT.csv\n",
      "Storm data saved for TISH.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\TISH.csv\n",
      "Storm data saved for VINI.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\VINI.csv\n",
      "Storm data saved for WASH.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WASH.csv\n",
      "Storm data saved for WATO.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WATO.csv\n",
      "Storm data saved for WAUR.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WAUR.csv\n",
      "Storm data saved for WEAT.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WEAT.csv\n",
      "Storm data saved for WEST.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WEST.csv\n",
      "Storm data saved for WILB.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WILB.csv\n",
      "Storm data saved for WIST.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WIST.csv\n",
      "Storm data saved for WOOD.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WOOD.csv\n",
      "Storm data saved for WYNO.csv to c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Storms_Identified_By_Increas_26\\WYNO.csv\n"
     ]
    }
   ],
   "source": [
    "# Define input and output directories\n",
    "path_long_term = os.path.join(os.getcwd(), \"Filtered Stations\")\n",
    "path_Storms_Identified_By_Increas = os.path.join(os.getcwd(), \"Storms_Identified_By_Increas_29\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(path_Storms_Identified_By_Increas, exist_ok=True)\n",
    "\n",
    "# Search for CSV files in the input directory\n",
    "pattern = os.path.join(path_long_term, '**/*.csv')\n",
    "csv_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Initialize a storm marker column\n",
    "    df['Storm_Marker'] = False\n",
    "\n",
    "    # Mark rows where RAIN increases from the previous value\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, 'RAIN'] > df.loc[i - 1, 'RAIN']:\n",
    "            df.loc[i, 'Storm_Marker'] = True\n",
    "            df.loc[i - 1, 'Storm_Marker'] = True  # Mark the starting point\n",
    "\n",
    "    # Filter the DataFrame to keep only rows marked as part of a storm\n",
    "    storm_df = df[df['Storm_Marker']].copy()  # Work on a copy to avoid warnings\n",
    "\n",
    "    # Check if there are any storms identified\n",
    "    if not storm_df.empty:\n",
    "        # Define the new file path in the output directory\n",
    "        relative_path = os.path.relpath(file_path, path_long_term)\n",
    "        new_file_path = os.path.join(path_Storms_Identified_By_Increas, relative_path)\n",
    "\n",
    "        # Create subdirectories in the output directory if they don't exist\n",
    "        os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "\n",
    "        # Save the storm data to a new CSV file\n",
    "        storm_df.drop('Storm_Marker', axis=1, inplace=True)  # Remove the marker column\n",
    "        storm_df.to_csv(new_file_path, index=False)\n",
    "\n",
    "        print(f\"Storm data saved for {os.path.basename(file_path)} to {new_file_path}\")\n",
    "    else:\n",
    "        print(f\"No storm identified in {os.path.basename(file_path)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb74f-4786-4a19-a86c-e87fea1e1ca2",
   "metadata": {},
   "source": [
    "### 1.1.4 Storm Separation\n",
    "This code consider that a 5-minute interval with a rainfall depth of zero that no rainfall event occurred. This code checks each row of the DataFrame to see if there is '5-min Rainfall (mm)' > 0. Encounting 5-min Rainfall is not > 0 (no rainfall events), the current storm is determined and it increments the storm_counter, repeating the process for the next storm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6895c5a-d026-453c-a174-25bc1b175986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input \n",
    "path_Storms_Identified_By_Increas = os.path.join(os.getcwd(), \"Storms_Identified_By_Increas_29\")\n",
    "# Find all CSV files in the input directory\n",
    "pattern = os.path.join(path_Storms_Identified_By_Increas, '*.csv')\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Define output\n",
    "path_single_storm = os.path.join(os.getcwd(), \"Single_Storm_29\")\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(path_single_storm, exist_ok=True)\n",
    "\n",
    "def process_and_save_rainfall_events(df, path_single_storm, station_id):\n",
    "    \"\"\"\n",
    "    Identify storm events starting from 0, followed by any values that do not decrease.\n",
    "    A storm ends just before the next value that is smaller or 0.\n",
    "    Each storm is saved in a separate file, starting from a 0 RAIN value and ending before a decrease or the next 0.\n",
    "    \"\"\"\n",
    "    event_number = 0\n",
    "    storm_start_index = None\n",
    "    in_storm = False\n",
    "\n",
    "    for i in range(len(df) - 1):\n",
    "        current_rain = df.loc[i, 'RAIN']\n",
    "        next_rain = df.loc[i + 1, 'RAIN']\n",
    "\n",
    "        # Start a new storm\n",
    "        if current_rain == 0 and next_rain > 0 and not in_storm:\n",
    "            storm_start_index = i\n",
    "            in_storm = True\n",
    "        # End the current storm\n",
    "        elif in_storm and (next_rain < current_rain or next_rain == 0):\n",
    "            event_number += 1\n",
    "            event_df = df.loc[storm_start_index:i]\n",
    "            event_filename = f\"{station_id}_event_{event_number}.csv\"\n",
    "            save_path = os.path.join(path_single_storm, f\"{station_id}\", event_filename)\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            event_df.to_csv(save_path, index=False)\n",
    "            in_storm = False\n",
    "            storm_start_index = i + 1 if next_rain == 0 else None\n",
    "\n",
    "    # Handle the case where the dataset ends while still in a storm\n",
    "    if in_storm:\n",
    "        event_number += 1\n",
    "        event_df = df.loc[storm_start_index:len(df) - 1]\n",
    "        event_filename = f\"{station_id}_event_{event_number}.csv\"\n",
    "        save_path = os.path.join(path_single_storm, f\"{station_id}\", event_filename)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        event_df.to_csv(save_path, index=False)\n",
    "\n",
    "    return {\"station_id\": station_id, \"events_saved\": event_number}\n",
    "\n",
    "# Process each file\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the station ID assuming 'STID' column exists\n",
    "    if 'STID' in df.columns:\n",
    "        station_id = df.iloc[0]['STID']  \n",
    "    else:\n",
    "        raise KeyError(f\"The file {file_path} does not contain 'STID' column.\")\n",
    "\n",
    "    # Process and save storm events\n",
    "    process_and_save_rainfall_events(df, path_single_storm, station_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a43ed3-a35c-4d72-b207-03bb05221275",
   "metadata": {},
   "source": [
    "## 1.2 Select Effective Storms\n",
    "Before identify the effective storms, this code calculates the accumulated rainfall (head named as 'Cumulative RAIN (mm)') for each storm by alculating the cumulative sum of the 5-minute interval rainfall depth ('5-min Rainfall (mm)').\n",
    "An effective storm is defined as a rainfall event that has little effect on water erosion since it has a low amount and intensity. In this section, the storm events with the Cumulative RAIN < 0.5 inches (12.5 mm) are deleted (AH ARS, 2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1f8d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process each CSV file\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m---> 15\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Check if cumulative rainfall exceeds 12.5 mm at any point\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define input \n",
    "path_single_storm = os.path.join(os.getcwd(), \"Single_Storm_29\")\n",
    "pattern = os.path.join(path_single_storm, '**/*.csv')\n",
    "csv_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# Define output\n",
    "path_effective_storm = os.path.join(os.getcwd(), \"Effective_Storms_29\")\n",
    "os.makedirs(path_effective_storm, exist_ok=True)\n",
    "\n",
    "# Dictionary to count storm events per station\n",
    "storm_event_counters = {}\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Check if cumulative rainfall exceeds 12.5 mm (0.5 inch) at any point\n",
    "    if df['RAIN'].max() < 0.5:\n",
    "        continue  # Skip if cumulative rainfall never exceeds 12.5 mm\n",
    "\n",
    "    # Determine station ID (use 'STID' if available, else use directory name)\n",
    "    station_id = df['STID'].iloc[0] if 'STID' in df.columns else os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "    # Create output directory for the station if it doesn't exist\n",
    "    output_dir_station = os.path.join(path_effective_storm, station_id)\n",
    "    if not os.path.exists(output_dir_station):\n",
    "        os.makedirs(output_dir_station)\n",
    "\n",
    "    # Increment the storm event counter for the current station\n",
    "    storm_event_counters[station_id] = storm_event_counters.get(station_id, 0) + 1\n",
    "\n",
    "    # Construct the new file name with the updated index\n",
    "    new_file_name = f\"{station_id}_event_{storm_event_counters[station_id]}.csv\"\n",
    "    new_file_path = os.path.join(output_dir_station, new_file_name)\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    df.to_csv(new_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20e384-7875-4899-88f2-e100c07b7dc0",
   "metadata": {},
   "source": [
    "## 1.3 Calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822588ac-f546-4861-ad12-6e4ad0321899",
   "metadata": {},
   "source": [
    "### 1.3.1 Calculate Intensity and Kinetic Energy \n",
    "This section is to\n",
    "1) calculate rainfall intensity for each 5-minute interval, which is an input in kinetic energy equation.\n",
    "2) find the maximum 30-minute intensity, also known as the maximum precipitation amount within 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd79b633-9794-439f-bae5-7d189c7b2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "path_effective_storm = os.path.join(os.getcwd(), \"Effective_Storms_29\")\n",
    "# Pattern to match all CSV files within the input directory\n",
    "pattern = os.path.join(path_effective_storm, '**/*.csv')\n",
    "# Find all files matching the pattern, including in subdirectories\n",
    "csv_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# output\n",
    "path_rainfall_intensity = os.path.join(os.getcwd(), \"Rainfall_intensity_29\")\n",
    "os.makedirs(path_rainfall_intensity, exist_ok=True)\n",
    "\n",
    "# Function to calculate the rainfall intensity and maximum intensity for an individual storm.\n",
    "def calculate_rainfall_intensity(df):\n",
    "    df['RAIN (mm)'] = df['RAIN'] * 25.4\n",
    "    df['Rainfall Interval (mm)'] = df['RAIN (mm)'].diff().fillna(df['RAIN (mm)'])\n",
    "    df['Rainfall Intensity (mm/h)'] = df['Rainfall Interval (mm)'] / df['Interval_minutes'] * 60\n",
    "    df['Unit energy (MJ/ha*mm)'] = 0.29 * (1 - 0.72 * np.exp(-0.082 * df['Rainfall Intensity (mm/h)']))\n",
    "    df['Energy in Interval (MJ/ha)'] = df['Unit energy (MJ/ha*mm)'] * df['Rainfall Interval (mm)'].astype(float)\n",
    "    \n",
    "    # Determine the amount of rainfall in 5-Min \n",
    "    df['5-Min Rainfall (mm)'] = df['Rainfall Interval (mm)'].fillna(0)\n",
    "    # Calculate the rainfall amount for each 10, 15, 20, 25, 30 min based on the amount of rainfall in 5-minute intervals:\n",
    "    # 10-Min = 2 * 5 min interval rainfall\n",
    "    # 15-Min = 3 * 5 min interval rainfall\n",
    "    # 20-Min = 4 * 5 min interval rainfall\n",
    "    # 25-Min = 5 * 5 min interval rainfall\n",
    "    # 30-Min = 6 * 5 min interval rainfall\n",
    "    for interval in [10, 15, 20, 25, 30]:\n",
    "        # Define interval as list, and execute the code for each item\n",
    "        column_name = f'{interval}-Min Rainfall (mm)'\n",
    "\n",
    "        # DataFrame.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=_NoDefault.no_default, closed=None, step=None, method='single')\n",
    "        # Provide rolling window calculations (more details in https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html).\n",
    "        # min_periods=1 is a parameter in .rolling () method in pandas.\n",
    "        # ensures that the rolling sum calculation returns a value even if the window is not fully populated,\n",
    "        # as long as there is at least one valid data point in the window.\n",
    "        df[column_name] = df['5-Min Rainfall (mm)'].rolling(window=interval//5, min_periods=1).sum()\n",
    "\n",
    "        # Fill the first rows corresponding to the interval with 0\n",
    "        # Save the calculated result in the row at {interval}-Min in f'{interval}-Min Rainfall (mm)' column\n",
    "        # Assign the previous row in f'{interval}-Min Rainfall (mm)' = 0.\n",
    "        # The value of this row is not produced from calculation, since the duration is less than 10-Min.\n",
    "        df.loc[0:interval//5 - 1, column_name] = 0\n",
    "    return df\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # # Filter rows where RAIN is less than 0.5\n",
    "    # df = df[df['RAIN'] >= 0.5]\n",
    "    # if df.empty:\n",
    "    #     # print(f\"File {os.path.basename(file_path)} ignored due to RAIN < 0.5.\")\n",
    "    #     continue  # Skip the rest of the loop for this file\n",
    "\n",
    "    # # Check if the maximum RAIN in the file is less than 0.5\n",
    "    # if df['RAIN'].max() < 0.5:\n",
    "    #     # print(f\"File {os.path.basename(file_path)} ignored due to max RAIN < 0.5.\")\n",
    "    #     continue  # Skip the rest of the loop for this file.\n",
    "        \n",
    "    # Convert the value in \"TIME\" column from string format to datetime objects using the pd.to_datetime() function\n",
    "    df[\"TIME\"] = pd.to_datetime(df[\"TIME\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calculate time differences and create the \"duration\" column\n",
    "    df[\"Interval\"] = df[\"TIME\"].diff()\n",
    "\n",
    "    # Convert the time interval to minutes\n",
    "    df['Interval_minutes'] = df['Interval'].dt.total_seconds() / 60\n",
    "    # df[\"Interval (hh:mm:ss)\"] = df[\"Interval\"].astype(str).str[-8:]\n",
    "    \n",
    "    # Calculate the rainfall intensity running the function \"calculate_rainfall_intensity(df)\"\n",
    "    df_processed = calculate_rainfall_intensity(df)\n",
    "\n",
    "    # Extract station name for subfolder creation\n",
    "    station_id = df_processed['STID'].iloc[0]\n",
    "    subfolder_path = os.path.join(path_rainfall_intensity, station_id)\n",
    "\n",
    "    # Create subfolder if it doesn't exist\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    # Define the new file path in the subfolder\n",
    "    new_file_path = os.path.join(subfolder_path, os.path.basename(file_path))\n",
    "\n",
    "    # Save the processed DataFrame to a new CSV file\n",
    "    df_processed.to_csv(new_file_path, index=False)\n",
    "    # print(f\"station {station_id} File saved: {new_file_path}\")\n",
    "\n",
    "print('All done!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5012f9-2cfa-4d77-88b3-a7073dd25075",
   "metadata": {},
   "source": [
    "### 1.3.2 Calculate rainfall erosivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea040f3-9735-4441-8c47-59ab2f5553e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output directories\n",
    "path_rainfall_intensity = os.path.join(os.getcwd(), \"Rainfall_intensity_29\")  # input \n",
    "path_rainfall_erosivity = os.path.join(os.getcwd(), \"Rainfall_erosivity_29\")  # output\n",
    "os.makedirs(path_rainfall_erosivity, exist_ok=True)\n",
    "\n",
    "# Find all the CSV files recursively in the input directory\n",
    "csv_files = glob.glob(os.path.join(path_rainfall_intensity, '**/*.csv'), recursive=True)\n",
    "\n",
    "# Function to extract station ID from the first 4 characters of the file name\n",
    "def extract_station_id(file_name):\n",
    "    return os.path.basename(file_name)[:4]\n",
    "\n",
    "# List to collect summary data for all storms across all stations\n",
    "summary_data = []\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract components and create new columns\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "    df['Month'] = df['TIME'].dt.month\n",
    "    df['Day'] = df['TIME'].dt.day\n",
    "    df['Year'] = df['TIME'].dt.year\n",
    "    df['Hour'] = df['TIME'].dt.hour\n",
    "    df['Minute'] = df['TIME'].dt.minute\n",
    "\n",
    "    Total_rainfall_inch = df['RAIN'].max()\n",
    "    Total_rainfall_mm = df['RAIN (mm)'].max()\n",
    "    Time = df['TIME']\n",
    "\n",
    "    # If the duration of a storm is less than 30 min, the I_30 is twice the amount of rain (AH 703).\n",
    "    rainfall_to_max_30_SI = df['RAIN (mm)'].max() * 2\n",
    "\n",
    "    # if the maximum 30 min rainfall is not equal to 0:\n",
    "    if df['30-Min Rainfall (mm)'].max() != 0:\n",
    "\n",
    "        # finds the index of the row with the maximum value in the \"30-Min Rainfall (mm)\"\n",
    "        max_row_SI = df['30-Min Rainfall (mm)'].idxmax()\n",
    "\n",
    "        # then retrieves the value from the 'RAIN (mm)' column at that index\n",
    "        rainfall_to_max_30_SI = df.at[max_row_SI, 'RAIN (mm)']\n",
    "\n",
    "        # calculate the maximum rainfall intensity with in 30 minutes\n",
    "        max_30_min_rainfall_intensity_SI = rainfall_to_max_30_SI * 60 / 30\n",
    "\n",
    "    # if the maximum 30-Min rainfall is = 0, return the default: the I_30 is twice the amount of rain\n",
    "    else:\n",
    "        max_30_min_rainfall_intensity_SI = rainfall_to_max_30_SI\n",
    "\n",
    "    # calculate the total energy of a storm\n",
    "    total_energy_SI = df['Energy in Interval (MJ/ha)'].sum()\n",
    "    \n",
    "    # calculate rainfall erosivity of a storm\n",
    "    rainfall_erosivity_SI = total_energy_SI * max_30_min_rainfall_intensity_SI\n",
    "\n",
    "    # Collect summary for each storm\n",
    "    for station_id in df['STID'].unique():\n",
    "        summary_data.append({\n",
    "            \"Storm File\": os.path.basename(file_path),\n",
    "            \"Station ID\": station_id,\n",
    "            \"TIME\":Time,\n",
    "            \"Total Rainfall (inch)\":Total_rainfall_inch,\n",
    "            \"Total Rainfall (mm)\": Total_rainfall_mm,\n",
    "            \"Rainfall @ Max 30-Min (mm)\": rainfall_to_max_30_SI,\n",
    "            \"Max 30-Min Intensity (mm/h)\": max_30_min_rainfall_intensity_SI,\n",
    "            \"Total Energy (MJ/ha)\": total_energy_SI,\n",
    "            \"Rainfall Erosivity ((MJ-mm)/(ha-hr))\": rainfall_erosivity_SI,\n",
    "            \"Day\": df['Day'].iloc[0],\n",
    "            \"Month\": df['Month'].iloc[0],\n",
    "            \"Year\": df['Year'].iloc[0],\n",
    "        })\n",
    "\n",
    "# Convert summary data to DataFrame and save for each station\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "unique_stations = summary_df['Station ID'].unique()\n",
    "\n",
    "for station_id in unique_stations:\n",
    "    station_summary = summary_df[summary_df['Station ID'] == station_id]\n",
    "    output_file_path = os.path.join(path_rainfall_erosivity, f\"{station_id.replace('/', '_')}.csv\")\n",
    "    station_summary.to_csv(output_file_path, index=False)\n",
    "    # print(f\"Saved erosivity data for station {station_id} to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33086f7-b928-44ac-b486-f3c9c8a0ebb3",
   "metadata": {},
   "source": [
    "### 1.3.3 Calculate Average Annual/Monthly R factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4013f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved in: c:\\Users\\mengting chen\\OneDrive - Oklahoma A and M System\\Mengtings Research Folder\\Erosion and Vulnerability Assessment\\R_estimation_gauge\\R_Computation\\Result_29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Input directory\n",
    "path_rainfall_intensity = os.path.join(os.getcwd(), \"Daily_rainfall_gauge_29\")  # Input folder containing CSV files\n",
    "\n",
    "# Output directory\n",
    "path_result = os.path.join(os.getcwd(), \"Result_29\")  # Output folder for results\n",
    "os.makedirs(path_result, exist_ok=True)\n",
    "\n",
    "# Initialize an empty DataFrame to hold combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Load and combine all files\n",
    "csv_files = glob.glob(os.path.join(path_rainfall_intensity, '*.csv'), recursive=True)\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d')  # Parse dates\n",
    "    df['Year'] = df['TIME'].dt.year\n",
    "    df['Month'] = df['TIME'].dt.month\n",
    "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "\n",
    "# Group by Month and Year to calculate monthly totals\n",
    "monthly_totals = combined_data.groupby(['STID', 'Year', 'Month'])['24hr RAIN (mm)'].sum().reset_index()\n",
    "\n",
    "# Calculate average monthly rainfall (over 29 years)\n",
    "average_monthly = monthly_totals.groupby(['STID', 'Month'])['24hr RAIN (mm)'].mean().reset_index()\n",
    "average_monthly.rename(columns={'24hr RAIN (mm)': 'Average Monthly Rainfall (mm)'}, inplace=True)\n",
    "\n",
    "# Pivot table to have months as columns\n",
    "average_monthly_pivot = average_monthly.pivot(index='STID', columns='Month', values='Average Monthly Rainfall (mm)')\n",
    "average_monthly_pivot.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Reset index for a clean output\n",
    "average_monthly_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Calculate annual totals\n",
    "annual_totals = monthly_totals.groupby(['STID', 'Year'])['24hr RAIN (mm)'].sum().reset_index()\n",
    "\n",
    "# Calculate average annual rainfall (over 29 years)\n",
    "average_annual = annual_totals.groupby('STID')['24hr RAIN (mm)'].mean().reset_index()\n",
    "average_annual.rename(columns={'24hr RAIN (mm)': 'Average Annual Rainfall (mm)'}, inplace=True)\n",
    "\n",
    "# Save results to the output folder\n",
    "average_monthly_pivot.to_csv(os.path.join(path_result, \"average_monthly_rainfall.csv\"), index=False)\n",
    "average_annual.to_csv(os.path.join(path_result, \"average_annual_rainfall.csv\"), index=False)\n",
    "\n",
    "print(f\"Processing complete. Results saved in: {path_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "# Output directory\n",
    "path_result = os.path.join(os.getcwd(), \"Result_29\")\n",
    "os.makedirs(path_result, exist_ok=True)\n",
    "\n",
    "# Input directory\n",
    "path_rainfall_erosivity = os.path.join(os.getcwd(), \"Rainfall_erosivity_29\")\n",
    "\n",
    "# Find all CSV files in the Rainfall Erosivity folder\n",
    "csv_files = glob.glob(os.path.join(path_rainfall_erosivity, '*.csv'), recursive=True)\n",
    "\n",
    "# Generate a list of month abbreviations (Jan, Feb, ..., Dec)\n",
    "month_names = [calendar.month_abbr[i] for i in range(1, 13)]\n",
    "\n",
    "# Initialize a list to hold data for each station\n",
    "station_results = []\n",
    "\n",
    "for file in csv_files:\n",
    "    # Extract station name from the filename\n",
    "    station_name = os.path.basename(file).replace('.csv', '')\n",
    "    \n",
    "    # Read the current file\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    # Define the long-term period\n",
    "    Period = 29  \n",
    "\n",
    "    # Calculate annual R factor and precipitation \n",
    "    annual_r_factor = data['Rainfall Erosivity ((MJ-mm)/(ha-hr))'].sum() / Period\n",
    "    annual_P_inch = data['Total Rainfall (inch)'].sum() / Period\n",
    "    annual_P_mm = data['Total Rainfall (mm)'].sum() / Period\n",
    "\n",
    "    # Calculate annual rainfall erosivity density\n",
    "    annual_density = (\n",
    "        annual_r_factor / annual_P_mm if annual_P_mm > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Initialize dictionaries for monthly calculations\n",
    "    monthly_r_factors = {}\n",
    "    monthly_P_inch = {}\n",
    "    monthly_P_mm = {}\n",
    "    monthly_contributions = {}\n",
    "    monthly_erosivity_density = {}\n",
    "    \n",
    "    for month_idx, month_name in enumerate(month_names):\n",
    "        # Filter data for the current month\n",
    "        monthly_data = data[data['Month'] == month_idx+1]\n",
    "        \n",
    "        # Monthly sums\n",
    "        monthly_r_sum = monthly_data['Rainfall Erosivity ((MJ-mm)/(ha-hr))'].sum()\n",
    "        monthly_P_inch_sum = monthly_data['Total Rainfall (inch)'].sum()\n",
    "        monthly_P_mm_sum = monthly_data['Total Rainfall (mm)'].sum()\n",
    "        \n",
    "        # Calculate monthly R factor, precipitation, and contributions\n",
    "        monthly_r_factors[f'{month_name} R'] = monthly_r_sum / Period\n",
    "        monthly_P_inch[f'{month_name} P(in)'] = monthly_P_inch_sum / Period\n",
    "        monthly_P_mm[f'{month_name} P(mm)'] = monthly_P_mm_sum / Period\n",
    "        monthly_contributions[f'{month_name} R contribution (%)'] = (\n",
    "            (monthly_r_sum / (Period * annual_r_factor) * 100) if annual_r_factor > 0 else 0\n",
    "        )\n",
    "        \n",
    "        # Calculate monthly rainfall erosivity density\n",
    "        monthly_erosivity_density[f'{month_name} R Density ((MJ-mm)/(ha-hr-mm))'] = (\n",
    "            monthly_r_sum / monthly_P_mm_sum if monthly_P_mm_sum > 0 else 0\n",
    "        )\n",
    "    \n",
    "    # Create a dictionary for the station with all calculated values\n",
    "    station_results.append({\n",
    "        'stid': station_name, \n",
    "        'Period (Years)': Period,\n",
    "        'Annual R Factor': annual_r_factor, \n",
    "        'Annual P (inch)': annual_P_inch,\n",
    "        'Annual P (mm)': annual_P_mm,\n",
    "        'Annual R Density ((MJ-mm)/(ha-hr-mm))': annual_density,\n",
    "        **monthly_r_factors,\n",
    "        **monthly_P_inch,\n",
    "        **monthly_P_mm,\n",
    "        **monthly_contributions,\n",
    "        **monthly_erosivity_density\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "station_results_df = pd.DataFrame(station_results)\n",
    "\n",
    "# Save the station-level results to a CSV file\n",
    "station_results_df.to_csv(os.path.join(path_result, 'R_factor_results.csv'), index=False)\n",
    "\n",
    "# Initialize a list for state results and calculate the average monthly R factor, contributions, and densities\n",
    "state_results = []\n",
    "state_density_results = []\n",
    "\n",
    "for month_name in month_names:\n",
    "    # State-level averages\n",
    "    avg_r = station_results_df[f'{month_name} R'].mean()\n",
    "    avg_contribution = station_results_df[f'{month_name} R contribution (%)'].mean()\n",
    "    avg_density = station_results_df[f'{month_name} R Density ((MJ-mm)/(ha-hr-mm))'].mean()\n",
    "    \n",
    "    state_results.append({\n",
    "        'Month': month_name, \n",
    "        'Monthly R Average': avg_r,\n",
    "        'Mean Contribution (%)': avg_contribution\n",
    "    })\n",
    "    \n",
    "    state_density_results.append({\n",
    "        'Month': month_name,\n",
    "        'Mean R Density ((MJ-mm)/(ha-hr-mm))': avg_density\n",
    "    })\n",
    "\n",
    "# Calculate average annual rainfall erosivity density across stations\n",
    "avg_annual_density = station_results_df['Annual R Density ((MJ-mm)/(ha-hr-mm))'].mean()\n",
    "\n",
    "# Save the average annual density to a CSV file\n",
    "state_density_results.append({\n",
    "    'Month': 'Annual',\n",
    "    'Mean R Density ((MJ-mm)/(ha-hr-mm))': avg_annual_density\n",
    "})\n",
    "\n",
    "# Convert the state results lists to DataFrames\n",
    "state_results_df = pd.DataFrame(state_results)\n",
    "state_density_df = pd.DataFrame(state_density_results)\n",
    "\n",
    "# Save the state results to CSV files\n",
    "state_results_df.to_csv(os.path.join(path_result, 'state_monthly_R_averages.csv'), index=False)\n",
    "state_density_df.to_csv(os.path.join(path_result, 'state_monthly_R_density_averages.csv'), index=False)\n",
    "\n",
    "print(\"Processing complete. Results saved to:\", path_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a0efa-35e7-4600-89bf-97cd1972069a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 Save the Calculated R to Mesonet Station file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9dd6b2-d093-452f-8ad8-d1613b7fc095",
   "metadata": {},
   "source": [
    "Operation this code require to: \n",
    "Installing arcpy package to work with jupyter notebook (The installation instraction step by step can be learnt from https://www.youtube.com/watch?v=DmqtYJ-liKU).\n",
    "\n",
    "In Part II have obtained results including long term average annual R factor, monthly R factor, and monthly contribution to annaul R factor for each site where recording precipiation over 10 years. However, those data are saved in \"R_factor_results.**csv**\" file. For geo-processing, this section save those data in **existed shapefile (named as \"ok_mesonet_sites_all_20210715.shp\")** recorded general information like station ID, station name, station coordinates, and elevation, etc., produced by Mesonet (https://mesonet.org/index.php/site/sites/mesonet_sites). \n",
    "\n",
    "This cell is using **AddJoin Tool** (https://pro.arcgis.com/en/pro-app/latest/tool-reference/data-management/add-join.htm) to save the calculated results in .csv into shapefile based on a common field, \"stid\", in this case. JoinField Tool also can join a layer to another layer or table based on a common field. However, the JoinField Tool will join a layer permanently, which can change the original shapefile. The original shapefile is not expected to be change, so that AddJoin Tool is applied in this case since the this tool joins a layer temporarily. According to copy the temporary layer, the joined layer can be saved as a permanet layer without changing the original layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb972e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary layer created.\n",
      "JoinField operation completed.\n",
      "New shapefile with joined data created.\n",
      "FID\n",
      "Shape\n",
      "stnm\n",
      "stid\n",
      "name\n",
      "city\n",
      "rang\n",
      "cdir\n",
      "cnty\n",
      "nlat\n",
      "elon\n",
      "elev\n",
      "cdiv\n",
      "clas\n",
      "WCR05\n",
      "WCS05\n",
      "A05\n",
      "N05\n",
      "BULK5\n",
      "GRAV5\n",
      "SAND5\n",
      "SILT5\n",
      "CLAY5\n",
      "TEXT5\n",
      "WCR10\n",
      "WCS10\n",
      "A10\n",
      "N10\n",
      "WCR25\n",
      "WCS25\n",
      "A25\n",
      "N25\n",
      "BULK25\n",
      "GRAV25\n",
      "SAND25\n",
      "SILT25\n",
      "CLAY25\n",
      "TEXT25\n",
      "WCR60\n",
      "WCS60\n",
      "A60\n",
      "N60\n",
      "BULK60\n",
      "GRAV60\n",
      "SAND60\n",
      "SILT60\n",
      "CLAY60\n",
      "TEXT60\n",
      "WCR75\n",
      "WCS75\n",
      "A75\n",
      "N75\n",
      "BULK75\n",
      "GRAV75\n",
      "SAND75\n",
      "SILT75\n",
      "CLAY75\n",
      "TEXT75\n",
      "datc\n",
      "datd\n",
      "stid_1\n",
      "Period__Ye\n",
      "Annual_R_F\n",
      "Annual_P__\n",
      "Annual_P_1\n",
      "Annual_R_D\n",
      "Jan_R\n",
      "Feb_R\n",
      "Mar_R\n",
      "Apr_R\n",
      "May_R\n",
      "Jun_R\n",
      "Jul_R\n",
      "Aug_R\n",
      "Sep_R\n",
      "Oct_R\n",
      "Nov_R\n",
      "Dec_R\n",
      "Jan_P_in_\n",
      "Feb_P_in_\n",
      "Mar_P_in_\n",
      "Apr_P_in_\n",
      "May_P_in_\n",
      "Jun_P_in_\n",
      "Jul_P_in_\n",
      "Aug_P_in_\n",
      "Sep_P_in_\n",
      "Oct_P_in_\n",
      "Nov_P_in_\n",
      "Dec_P_in_\n",
      "Jan_P_mm_\n",
      "Feb_P_mm_\n",
      "Mar_P_mm_\n",
      "Apr_P_mm_\n",
      "May_P_mm_\n",
      "Jun_P_mm_\n",
      "Jul_P_mm_\n",
      "Aug_P_mm_\n",
      "Sep_P_mm_\n",
      "Oct_P_mm_\n",
      "Nov_P_mm_\n",
      "Dec_P_mm_\n",
      "Jan_R_cont\n",
      "Feb_R_cont\n",
      "Mar_R_cont\n",
      "Apr_R_cont\n",
      "May_R_cont\n",
      "Jun_R_cont\n",
      "Jul_R_cont\n",
      "Aug_R_cont\n",
      "Sep_R_cont\n",
      "Oct_R_cont\n",
      "Nov_R_cont\n",
      "Dec_R_cont\n",
      "Jan_R_Dens\n",
      "Feb_R_Dens\n",
      "Mar_R_Dens\n",
      "Apr_R_Dens\n",
      "May_R_Dens\n",
      "Jun_R_Dens\n",
      "Jul_R_Dens\n",
      "Aug_R_Dens\n",
      "Sep_R_Dens\n",
      "Oct_R_Dens\n",
      "Nov_R_Dens\n",
      "Dec_R_Dens\n",
      "Records with R factor = 0 removed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path to your ZIP file\n",
    "zip_file_path = os.path.join(os.getcwd(), \"mesonet_sites_shape.zip\")\n",
    "# zip_file_path = os.path.join(base_dir,\"Geoprocess_inputs\" ,\"mesonet_sites_shape.zip\")\n",
    "\n",
    "# Destination directory where the contents of the zip will be extracted\n",
    "extract_to_directory =os.path.join(os.getcwd(),\"Geoprocess_inputs\")\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(extract_to_directory, exist_ok=True)\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_directory)\n",
    "\n",
    "# Paths to the shapefile and the join table CSV\n",
    "shapefile_path = os.path.join(os.getcwd(),\"Geoprocess_inputs\", \"mesonet_sites_shape\",\"ok_mesonet_sites_all_20210715.shp\")\n",
    "join_table = os.path.join(os.getcwd(), \"Result_29\", \"R_factor_results.csv\")\n",
    "output_dir = os.path.join(os.getcwd(), \"shapefile_outputs\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "arcpy.env.workspace = output_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# The qualifiedFieldNames environment is used by Copy Features when persisting the join field names.\n",
    "arcpy.env.qualifiedFieldNames = False\n",
    "\n",
    "\n",
    "# Process: Create a Feature Layer from the shapefile\n",
    "temp_layer_sites = \"mesonet_sites_layer\"\n",
    "arcpy.MakeFeatureLayer_management(shapefile_path, temp_layer_sites)\n",
    "print(\"Temporary layer created.\")\n",
    "\n",
    "# Process: Add Join\n",
    "joined_table_temp_layer = arcpy.management.AddJoin(temp_layer_sites, \"stid\", join_table, \"stid\")\n",
    "print(\"JoinField operation completed.\")\n",
    "\n",
    "# Process: Copy Features to new shapefile\n",
    "joined_shapefile_path = os.path.join(output_dir, \"MesoSite_R.shp\")\n",
    "arcpy.CopyFeatures_management(joined_table_temp_layer, joined_shapefile_path)\n",
    "print(\"New shapefile with joined data created.\")\n",
    "\n",
    "# Process: List and print field names in the new shapefile\n",
    "fields = arcpy.ListFields(joined_shapefile_path)\n",
    "for field in fields:\n",
    "    print(field.name)\n",
    "\n",
    "# Process: Remove records where Annual R Factor = 0\n",
    "with arcpy.da.UpdateCursor(joined_shapefile_path, ['Annual_R_F']) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] == 0:\n",
    "            cursor.deleteRow()\n",
    "print(\"Records with R factor = 0 removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c805b92-ba9e-4951-8762-2078d04b7234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
